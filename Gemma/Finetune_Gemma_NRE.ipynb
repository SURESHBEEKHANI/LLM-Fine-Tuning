{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Finetune_Gemma_NRE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWJhJKWnSui3"
      },
      "source": [
        "### **Install needed packages**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3WjLPjwhSj2k"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "# Install Flash Attention 2 for softcapping support\n",
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGylRbdfJilw"
      },
      "source": [
        "### **Loading a Pre-trained Language Model with Custom Configuration for Efficient Memory Usage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "dfbf34b7cabb452aa1bd4275ad5ca478",
            "c82d5b1f1ea04e4697624601a83308ad",
            "9752169445144247bbaab60d8234ea93",
            "56dbb978bc054c74b87b8a7cfcc01627",
            "4301e4c73e964e35bb836df27a476141",
            "352a9b4eef5b414787eb87b8b1983c57",
            "503adcd44b9948c7ba9a47345d08b629",
            "a847cb0e19c64554832cafb7794c2169",
            "f0883a7439dd469aa0ecb61be34939f2",
            "97dae5b1902d4d3ba163e60094e14cfd",
            "0c7d317541bd40b7923f7543e44df9c2",
            "3d6f676289e64df7b5755600522273ed",
            "db4d663c42a249f4b4146ae1c72fdf7b",
            "4da48ce973fa41218122e6290918752e",
            "5ea382ab0f684e7eaede06db9ab6dcc5",
            "488ef5bdf49a49ad920e7874ce8bbf5d",
            "f054ab5996de4f1393ea688ffacb88ab",
            "cdcfff298ba445ddbc1fad739d003cab",
            "7f6f928a1ced44e6ab1356f99ba36558",
            "708be012414b4279a2fedfd39a03acac",
            "e74859d955144b3e975991245ebcbb51",
            "93c65e600a164e13abcf0f34abe817a1",
            "a41c6110e6dd418c8bdc720c21491c01",
            "7cc8e1de06c74562aef8b1214eac2ca6",
            "1b45412bbd664b37b2c5db628c74e9e1",
            "0956649daa4e4a33a0637955ceb3af4d",
            "329935d1a4d74bad95c68c8bfa72d0a3",
            "063f95215ce24d71b297f4c4360e6225",
            "e905ab3258894707b8e2e0468f79da50",
            "d1e305724c21436eb29f8d3adab15b43",
            "5ea8a5a7dc7d4468b8dcef1dbc00dc2b",
            "b07674343c434449a129e150c7ea6a30",
            "82b7da1cf0714fcf91f48fd402cf6831",
            "b595281ea8eb4527b5b47d26d6fe96e6",
            "6f21f3c31fec4c7290c551f54c67a0cb",
            "adf3170238e04fc8a6dc993e47a7962b",
            "5e43b7ae820b47ccbeb275fff80876a1",
            "2c0e825947f74d2fb4a55ba0af11664e",
            "ded6c48929264aac844be5332d48c8aa",
            "5082df8250f2490793ae4ef2f199da19",
            "d56b875834e34ebe81df3fb1ca00e06a",
            "f4ed10971d33485d8c2d58ad16f8b98a",
            "5bb4b8ce480b4197ba85ffc6c21f629b",
            "b35fb25485304c46a32536c9d7b76be8",
            "e7aa18171eed429a9d47ca112031c6e3",
            "991ad7e9c0b1436f9658329e5bc58f8a",
            "3b2001049d094d21b777d822fd67eb78",
            "6d362fbf445045389a19c301caaa0d48",
            "6bebe9bf15d84ca6a2ca01e869449c5b",
            "80440b8e7e6c410ba73218ebb3132860",
            "851b8a34172d437a87bd7b9b88b12193",
            "cc42a293ae70496a873b9067756520fa",
            "3d57ce7bad734fecbde8468d93890c66",
            "8476aeb8c37842f98033e718d6e49898",
            "bb7249b8b40b489b934c31638afb166a",
            "448a7a04fa6c47af8952cab7ede820bd",
            "13c0e7f5b76d438185e8e99ab3855a3c",
            "f784f27dd5554d4ab36f533e6251d259",
            "add1218f050c4b87bd3c836e564e136a",
            "045eed819ff4464e8c120a9edd46681d",
            "103c8c86485042798569851ef45e6b10",
            "fa99e8d6359a4e97a37a27e538b9a7c8",
            "268d6b2a547542a7a5daf679c4f40f7f",
            "9d83d1457b4c40f68630c65c7628d11f",
            "041ef4cee1c8404389d65eddcc1f0a79",
            "01f2d0aadcc34dbaa6db45126e7f0a9b"
          ]
        },
        "id": "H2EJLqgyJyGf",
        "outputId": "c2b3c1f9-a0a3-4072-8bed-aaa604ec8308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.1.7: Fast Gemma2 patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfbf34b7cabb452aa1bd4275ad5ca478",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d6f676289e64df7b5755600522273ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a41c6110e6dd418c8bdc720c21491c01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b595281ea8eb4527b5b47d26d6fe96e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7aa18171eed429a9d47ca112031c6e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "448a7a04fa6c47af8952cab7ede820bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from unsloth import FastLanguageModel  # FastLanguageModel provides easy loading of pre-trained models.\n",
        "import torch  # PyTorch library for tensor computations and deep learning\n",
        "\n",
        "# Configuration settings for the model\n",
        "max_seq_length = 2048  # Define the maximum sequence length (2048 tokens in this case).\n",
        "# The model can handle larger sequence lengths, and RoPE (Rotary Positional Embedding) scaling will be applied internally.\n",
        "\n",
        "dtype = None  # The data type of the model's parameters. None means auto-detection.\n",
        "# If you're using a Tesla T4 or V100, you might use Float16 for better performance.\n",
        "# For Ampere+ (like A100, V100), Bfloat16 is usually a better option.\n",
        "\n",
        "load_in_4bit = True  # If set to True, 4-bit quantization is applied to the model weights, reducing memory usage.\n",
        "# This is useful for low-memory environments or when working with large models. Set to False to use full precision.\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2-9b\",  # Specify the name of the pre-trained model to load (in this case, \"gemma-2-9b\").\n",
        "    max_seq_length = max_seq_length,  # Pass the defined maximum sequence length.\n",
        "    dtype = dtype,  # Pass the dtype configuration (None for auto-detection).\n",
        "    load_in_4bit = load_in_4bit,  # Pass the flag for using 4-bit quantization.\n",
        "    # token = \"hf_...\",  # Uncomment and provide a token if using gated models like meta-llama (for example, Llama-2-7b-hf).\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_dGT6z9LAf-"
      },
      "source": [
        "### **We now add LoRA adapters so we only need to update 1 to 10% of all parameters!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7dWgo6E8GO",
        "outputId": "b5ada06a-17cb-4cd6-9e95-ac515a1c3636"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.1.7 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
          ]
        }
      ],
      "source": [
        " # Configure the model with PEFT (Parameter-Efficient Fine-Tuning) settings using LoRA (Low-Rank Adaptation)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The base model to be fine-tuned using PEFT techniques\n",
        "\n",
        "    # Low-Rank Adaptation (LoRA) rank\n",
        "    r=16,  # Defines the rank of the low-rank matrices. Common choices: 8, 16, 32, 64, 128.\n",
        "    # Larger values increase expressiveness but require more memory.\n",
        "\n",
        "    # Modules to target for LoRA fine-tuning\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projection layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "    ],\n",
        "    # Only these specified modules will be fine-tuned to reduce memory and computational overhead.\n",
        "\n",
        "    # LoRA-specific hyperparameters\n",
        "    lora_alpha=16,  # Scaling factor for LoRA weights. Balances new and pre-trained weights.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA. Setting to 0 often gives optimized performance.\n",
        "\n",
        "    # Bias handling in fine-tuning\n",
        "    bias=\"none\",  # Specifies bias tuning. \"none\" is optimized for performance. Alternatives: \"all\", \"lora_only\".\n",
        "\n",
        "    # Optimizations for VRAM and context length\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Use gradient checkpointing to save memory during training.\n",
        "    # The \"unsloth\" setting reduces VRAM usage by ~30%, allowing larger batch sizes or longer contexts.\n",
        "\n",
        "    # Random seed for reproducibility\n",
        "    random_state=3407,  # Ensures the results are reproducible across runs.\n",
        "\n",
        "    # Advanced fine-tuning features\n",
        "    use_rslora=False,  # Enables Rank-Stabilized LoRA (rSLoRA) if set to True. Useful for stability in high ranks.\n",
        "    loftq_config=None,  # Configures LoftQ (Low Overhead Fine-Tuning Quantization), if used. Set to None for default.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb0_jF9WAAja"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Preparation\n",
        "We are using the **Named Entity Recognition (NER)** dataset from [SURESHBEEKHANI](https://huggingface.co/datasets/SURESHBEEKHANI/Named_entity_recognition). This dataset is ideal for training on named entity recognition tasks, where the goal is to identify entities such as person names, locations, and organizations within the text.\n",
        "\n",
        "You can replace the dataset loading section with your own data preparation steps, depending on your specific use case.\n",
        "\n",
        "**[NOTE]** To train only on the output (ignoring any extra context like the user's input), please refer to [TRL's documentation](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Itâ€™s important to add the **EOS_TOKEN** to the tokenized output. Without it, your model could generate an infinite sequence! This marks the end of the generated text and helps control the length of the output.\n",
        "\n",
        "For training on conversational datasets, you may want to use the `llama-3` template. We have prepared a conversational notebook that you can find [here](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "If your goal is to work with text completions (such as for creative writing), consider using this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "bc0cf86bb9cc4cd587b32865f966642a",
            "23830c112a784c429c087b7615558a7a",
            "58fc1e95a1674f54b055343605260948",
            "cabdd72ad80c4a468fe45256d2a76557",
            "d8de2f964c0b42ab8edc05b0e0a58430",
            "88e5ea8ea7e344609418034a6337a44b",
            "bb578bc7bba7408a9c022ef411edb0d0",
            "f8723223377b4d09a00e55d06f3f703c",
            "b1a7410a679d47149e7ff6e5a3c7c453",
            "c1cffd8c2d75427ab5fdf6d280cef2ba",
            "ccec976c24eb40d8aa959766e99983a3",
            "399b396ee7fb43b096133dc75d4b9df0",
            "40bcdc93adb74c4baea2e1bdda524362",
            "abd40393f5354cbeb2c53e33c56d87c6",
            "eb3b328104954285bf694de63e6c0d35",
            "fa4a36ca0dc84195add7474f718b3620",
            "281a8a9436cd452788ab5e7c87d46aa7",
            "13fbcabf3fe84cecb8c7cc9e1f49fe98",
            "3f47f0a94d184993a2f85a7c9ca44271",
            "e754ccb651a045b3a7ad7ba01a1719ad",
            "0d41e65fe3624c57a4c42bcdb4e99a7d",
            "6170800399c248c3b6c83ada8754bd21",
            "632d465ebf1c4876975641eb1a2ca45a",
            "207341d8b1104761b4da223d4cafbf4d",
            "c21ca3aa13414ffeb39e68878e9e3ab0",
            "bd8a75aa002e499f99092e0c1b5f57f6",
            "61cbb48762854c72ad83cc906139b596",
            "c0c4aa3cd39b43899605f73dcd2d1459",
            "ca0e08570b784c878e49c141239a7089",
            "867c131d0fcf4700a75a95e4d4c375b7",
            "78949efd2d01419dace7c425a2709b53",
            "39d6be827aa64d6e9b1e3d7d55033333",
            "c28a257b625940c8935732051fb21305",
            "ff717cf5ae304c5d94c867a4d3fde006",
            "dffffc4858c842acb1b692a8b7f3b82b",
            "e61e9a275c46493f9ea3c44f82afe097",
            "a1ea12d52a734a8380db5ef0f5fdec28",
            "1e1eee19e4124d1eb95a6a83e7e596c1",
            "152d2743aa7747548871d293ebae1bb7",
            "0bb68ef07ba14a41b2a8c25d47458586",
            "90467e60959a49ecb016e8b7040b7e18",
            "dec514a776774217ad4f1bcfd78a6442",
            "415d5bd0b30f4c3daea62bec20e8242f",
            "ef2eaca87f2a4e8f8f74a4772ea5a09c"
          ]
        },
        "id": "AcLqLN7n_kmC",
        "outputId": "f5bcda15-cf42-4774-8df8-347a5d09bb73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc0cf86bb9cc4cd587b32865f966642a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "399b396ee7fb43b096133dc75d4b9df0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/1.76M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "632d465ebf1c4876975641eb1a2ca45a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/11989 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff717cf5ae304c5d94c867a4d3fde006",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/11989 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the prompt template for Alpaca-based task instruction\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "# The alpaca_prompt template includes placeholders for instruction, input, and output. These will be filled dynamically during processing.\n",
        "\n",
        "# EOS_TOKEN (End of Sequence Token) to signal the end of a generated sequence. It's necessary to prevent infinite generation.\n",
        "EOS_TOKEN = tokenizer.eos_token  # Retrieve the EOS token from the tokenizer.\n",
        "\n",
        "# Define a function to format the dataset examples into the Alpaca prompt format\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instructions\"]  # Extract instructions from the dataset.\n",
        "    inputs = examples[\"input\"]  # Extract input data from the dataset.\n",
        "    outputs = examples[\"Output\"]  # Extract the expected output from the dataset.\n",
        "\n",
        "    texts = []  # Initialize a list to store the formatted prompt texts.\n",
        "\n",
        "    # Loop through each instruction, input, and output in parallel.\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Format the alpaca_prompt with the current instruction, input, and output.\n",
        "        # EOS_TOKEN is added to signal the end of the text generation.\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)  # Append the formatted text to the list.\n",
        "\n",
        "    # Return the formatted texts in a dictionary with the key \"text\".\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Load a dataset from Hugging Face's datasets library\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"SURESHBEEKHANI/Named_entity_recognition\", split=\"train\")\n",
        "# This loads the \"train\" split of the \"Named_entity_recognition\" dataset by the user \"SURESHBEEKHANI\".\n",
        "\n",
        "# Apply the formatting function to the dataset using the `map` method.\n",
        "# This will apply the `formatting_prompts_func` to each example in the dataset in a batched manner.\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RbeMMMUBL0J"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the Model\n",
        "Next, let's train the model using Hugging Face TRL's `SFTTrainer`! For more details, check out the [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer).\n",
        "\n",
        "To speed up the process, we perform 100 steps in this example, but you can modify the `num_train_epochs` parameter to `1` for a full training run and set `max_steps=None` for training based on the number of epochs instead.\n",
        "\n",
        "Additionally, we also support using TRL's `DPOTrainer` for other fine-tuning strategies, which you can explore if needed!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d84311ce64b04f1cb8289f5913a8f3db",
            "4c9239698b2b4a6ebca119a9be635b35",
            "1afd013244ba4d0288027e175de5b374",
            "8fe3dc385d894955bb64c0c3daf723b1",
            "085150c68013475aa4f9cc1a2659b4f5",
            "10a7c264c6544f35962ef351bdea8a2a",
            "8384043d99874338b3470d281ad11188",
            "9f8a28665b7d4b049e61b3c54ba90e73",
            "87965ed43d0049abbf8487fb4044499c",
            "10fdbcdcca73414da52f6db03054aece",
            "5135a507e07744eea8549164aae45c22"
          ]
        },
        "id": "jjXierjMAVvk",
        "outputId": "fbf7adf4-0f68-426f-bfe6-a3d210e44bff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d84311ce64b04f1cb8289f5913a8f3db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/11989 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from trl import SFTTrainer  # SFTTrainer is used for training models in a parameter-efficient way.\n",
        "from transformers import TrainingArguments  # TrainingArguments contains configuration options for model training.\n",
        "from unsloth import is_bfloat16_supported  # Function to check if the hardware supports bfloat16 precision.\n",
        "\n",
        "# Initialize the SFTTrainer with the necessary arguments\n",
        "trainer = SFTTrainer(\n",
        "    model = model,  # The pre-trained model to be fine-tuned.\n",
        "    tokenizer = tokenizer,  # The tokenizer used to process input data for the model.\n",
        "    train_dataset = dataset,  # The dataset to train on, assumed to be preprocessed and formatted.\n",
        "    dataset_text_field = \"text\",  # Specifies which field in the dataset contains the input text. Here, it's the \"text\" field.\n",
        "    max_seq_length = max_seq_length,  # The maximum length of input sequences for the model. Ensures sequences are not longer than this value.\n",
        "    dataset_num_proc = 2,  # Number of CPU processes to use for data loading. 2 can speed up the dataset loading process.\n",
        "    packing = False,  # If set to True, sequences will be packed into a single tensor (can make training faster for short sequences).\n",
        "\n",
        "    # Define the training configuration through TrainingArguments\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,  # Batch size per device during training.\n",
        "        gradient_accumulation_steps = 4,  # Number of steps to accumulate gradients before performing an update (helps with memory efficiency).\n",
        "        warmup_steps = 5,  # Number of steps to perform learning rate warmup before training starts.\n",
        "        max_steps = 50,  # The total number of training steps. Once reached, training will stop.\n",
        "        learning_rate = 2e-4,  # Learning rate for the optimizer.\n",
        "\n",
        "        # fp16 (16-bit floating point) and bf16 (bfloat16) are precision modes used to speed up training and reduce memory usage.\n",
        "        fp16 = not is_bfloat16_supported(),  # Use fp16 if bfloat16 is not supported by the hardware.\n",
        "        bf16 = is_bfloat16_supported(),  # Use bf16 if supported by the hardware (typically for Ampere GPUs or newer).\n",
        "\n",
        "        logging_steps = 1,  # Log training progress every step. Setting this to a higher value reduces logging frequency.\n",
        "        optim = \"adamw_8bit\",  # Specifies the optimizer used during training (AdamW with 8-bit precision for memory efficiency).\n",
        "        weight_decay = 0.01,  # Regularization parameter to prevent overfitting by penalizing large weights.\n",
        "        lr_scheduler_type = \"linear\",  # Learning rate scheduler type. \"linear\" gradually decays the learning rate during training.\n",
        "        seed = 3407,  # Random seed for reproducibility of results.\n",
        "        output_dir = \"outputs\",  # Directory to save model checkpoints and logs during training.\n",
        "        report_to = \"none\",  # Specifies where to report metrics (e.g., \"none\" means no reporting, or use \"wandb\" for Weights & Biases).\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMNXjrsNB7qF",
        "outputId": "2bfc80ee-5d45-46d3-8515-e39f1d8d145e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "6.879 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "189zoApYBqgI",
        "outputId": "e1a85322-d7fe-405f-9a1f-f76a5aa27741"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 11,989 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 50\n",
            " \"-____-\"     Number of trainable parameters = 54,018,048\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 14:32, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.609300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.436900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.529800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.446900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.298200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.034900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.881200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.720400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.696200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.641800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.642400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.593900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.608400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.554200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.607800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.638400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.545600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.583900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.582100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.523700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.579900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.550600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.565000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.565700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.506700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.546500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.528000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.565300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.544600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.541100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.536900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.538200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.551300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.543100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.527700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.526300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.539000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.500600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.549100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.549600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.538000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.565700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.546200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.531900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.539800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.511000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.555800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.530700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.528800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.569900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq6rqXmsCBSu",
        "outputId": "55aeab28-496e-423a-8d30-f28a5d1811df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "928.6021 seconds used for training.\n",
            "15.48 minutes used for training.\n",
            "Peak reserved memory = 13.617 GB.\n",
            "Peak reserved memory for training = 6.738 GB.\n",
            "Peak reserved memory % of max memory = 92.331 %.\n",
            "Peak reserved memory for training % of max memory = 45.688 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phRIcf7JCoNk"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUFi-IboCIse",
        "outputId": "e4387636-3813-4ceb-94ee-eb4aef3ab8b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<bos>Your AI assistant for NER\", \"The entities are categorized into different types, such as PERSON, LOCATION, ORGANIZATION, etc.\", \"Please review the extracted entities for any potential errors or misclassifications.\", \"For improved accuracy, try using a more domain-specific NER mode.\\n\\n### input:\\nOn the Republican side , Senator John McCain seems on the verge of clinching his party \\'s nomination \\n\\n### Output:\\n[{\\'end\\': 10, \\'entity\\': \\'I-PER\\', \\'index\\': 2, \\'score\\': 0.99999994, \\'start\\': 7, \\'word\\': \\'John\\'}, {\\'end\\': 13, \\'entity\\': \\'I-PER\\',']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " #Define the prompt template for text summarization.\n",
        "alpaca_prompt = \"\"\"Your AI assistant for NER\", \"The entities are categorized into different types, such as PERSON, LOCATION, ORGANIZATION, etc.\", \"Please review the extracted entities for any potential errors or misclassifications.\", \"For improved accuracy, try using a more domain-specific NER mode.\n",
        "\n",
        "### input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"  # The summary part is left empty for generation.\n",
        "\n",
        "# FastLanguageModel.for_inference(model) enables optimizations for faster inference.\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference by configuring the model for efficient use\n",
        "\n",
        "# Example of a text summarization task.\n",
        "# Here, you provide a longer piece of text as input, and the model will generate a concise summary.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "         alpaca_prompt.format(  # Format the prompt with the input text and an empty placeholder for the summary.\n",
        "            \"\"\"On the Republican side , Senator John McCain seems on the verge of clinching his party 's nomination \"\"\"\n",
        "            ,  # Insert input text for summarization.\n",
        "            \"\"  # The summary section is empty for the model to fill in.\n",
        "        )\n",
        "    ], return_tensors=\"pt\"  # Convert input to PyTorch tensors.\n",
        ").to(\"cuda\")  # Move the input data to the GPU for faster processing.\n",
        "\n",
        "# Generate the summary using the model.\n",
        "# 'max_new_tokens' controls how many tokens the model is allowed to generate.\n",
        "# 'use_cache' allows for faster generation by caching previous results.\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "# Decode the generated tokens back into readable text.\n",
        "# This will give us the model's summary of the provided input text.\n",
        "tokenizer.batch_decode(outputs)  # Convert the output tokens to text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71NTd17ECyQU",
        "outputId": "64a37dc5-fa93-4f61-e288-ea93c55b0dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos>Your AI assistant for NER\", \"The entities are categorized into different types, such as PERSON, LOCATION, ORGANIZATION, etc.\", \"Please review the extracted entities for any potential errors or misclassifications.\", \"For improved accuracy, try using a more domain-specific NER mode.\n",
            "\n",
            "### input:\n",
            "On the Republican side , Senator John McCain seems on the verge of clinching his party 's nomination \n",
            "\n",
            "### Output:\n",
            "[{'end': 10, 'entity': 'I-PER', 'index': 2, 'score': 0.99999994, 'start': 7, 'word': 'John'}, {'end': 13, 'entity': 'I-PER', 'index': 3, 'score': 0.99999994, 'start': 11, 'word': 'Mc'}, {'end': 15, 'entity': 'I-PER', 'index': 4, 'score': 0.999\n"
          ]
        }
      ],
      "source": [
        "#Define the prompt template for text summarization.\n",
        "alpaca_prompt = \"\"\"Your AI assistant for NER\", \"The entities are categorized into different types, such as PERSON, LOCATION, ORGANIZATION, etc.\", \"Please review the extracted entities for any potential errors or misclassifications.\", \"For improved accuracy, try using a more domain-specific NER mode.\n",
        "\n",
        "### input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"  # The summary part is left empty for generation.\n",
        "\n",
        "# FastLanguageModel.for_inference(model) enables optimizations for faster inference.\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference by configuring the model for efficient use\n",
        "\n",
        "# Example of a text summarization task.\n",
        "# Here, you provide a longer piece of text as input, and the model will generate a concise summary.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "         alpaca_prompt.format(  # Format the prompt with the input text and an empty placeholder for the summary.\n",
        "            \"\"\"On the Republican side , Senator John McCain seems on the verge of clinching his party 's nomination \"\"\"\n",
        "            ,  # Insert input text for summarization.\n",
        "            \"\"  # The summary section is empty for the model to fill in.\n",
        "        )\n",
        "    ], return_tensors=\"pt\"  # Convert input to PyTorch tensors.\n",
        ").to(\"cuda\")  # Move the input data to the GPU for faster processing.\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Initialize the TextStreamer to decode the generated tokens during streaming.\n",
        "# This facilitates immediate feedback on the modelâ€™s output.\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Generate the summary using the model, streaming token-by-token for faster results.\n",
        "# The model will produce a summary up to a maximum of 128 tokens.\n",
        "_ = model.generate(\n",
        "    **inputs,  # Provide the tokenized input text to the model.\n",
        "    streamer=text_streamer,  # Enable token-by-token streaming.\n",
        "    max_new_tokens=128  # Limit the number of tokens in the generated summary.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJicn-DqDBpU"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjRsm4U0C8sM",
        "outputId": "f22feb24-fbec-45cc-8d71-f1e76cd13c70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.model',\n",
              " 'lora_model/added_tokens.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_okKJHSD5Ra"
      },
      "source": [
        "##### Push the trained model to the Hugging Face Model Hub using the GGUF format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e1a83d9c10f54e28b318e05f575f2708",
            "d4f03d6351604849b4a74dfcbb6cfd52",
            "0f4098aa9dec44f08ed235b1502849b6",
            "211dba39ce344a83a77f936203b56498",
            "4bdc189a1e9f4397b07371e067b9a179",
            "3f423b836fa54e229fd971bc1b91d7cf",
            "ef6e8f97c4e84aebad4838fb6728205c",
            "29c63312ded547a190e166c0a256e35e",
            "c3b6afd0d1b24406bb7e5ebd234d624c",
            "2eade1693f3948e2a5d74d7bd09995f2",
            "03c68aaa143145788d61d1583b5e302f",
            "975ce87429314387b4fca236ebcfca61",
            "feede8689d514e1ba0daf3bb0cf63ce0",
            "6538fc08435e449694e110e94fd760c1",
            "e2efff78e8f24c419ad790f5ce528a5f",
            "26ee0f8c8739433b87383d27def8188d",
            "aec3cabe68054801b3bde5aa24e90759",
            "b11f434f0dee4c59bf5ba578ae73293b",
            "2ea78ce8d6ce43cb8ef96319fd11eb9f",
            "76c59403e82447ecb393aac16d665158",
            "f891fc5a851946daa9ff6490ef074ded",
            "4c4b3295658e49e3b3907ef93d02d76c",
            "af6a61f055474681b43bfba5e6cdd21d",
            "aa980ef1d818481297d1670c0cd2060e",
            "bbc49154016a49e68d7159b5a52fb4d9",
            "be50c673005a43de91e49ec7cbee2d1d",
            "bbe25740523a4b2f8894c048632c82e4",
            "e46d56d74db04e9e9919cad8a9dd6b2e",
            "2703a6d091f147ebaf3ba55e90939dfa",
            "a5675d7e6ac64961aba2dae32245d3b5",
            "8375c78d595645cfaeb49b70fa3655c0",
            "1ab4050223e046eeae80de31713edfb9",
            "250e03adeaff4799b76616c5d626b900",
            "c1aa0730f9d543d6bcc46944010bf38f",
            "f0871295f2674d8b88043b8ad2426cbb",
            "159b71d1e4814e5da5c3aa382ad105ac",
            "8dd18e28c177447188345dfbf22db8b8",
            "4b1a5f524805476d98be6a20c827ffce",
            "b99b0592a528481ebfcdae0d2c662247",
            "40da6317ee3a4419b893ab20ed61b1a0",
            "72d4f8be0ff54355a7cec30cf9be09fc",
            "f4d11098ccc049558178fea91da32637",
            "14b3cd3a7f2f4770b15cf4b044bc139c",
            "81514250d1a9423da15d275e2f2026d2"
          ]
        },
        "id": "CQGcMr4wD6AS",
        "outputId": "b0cdd7f7-b282-42c2-f581-23166bfe2741"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 6.1G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 4.75 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|â–ˆâ–ˆâ–ˆ       | 13/42 [00:01<00:02, 13.91it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [03:10<00:00,  4.54s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/pytorch_model-00004-of-00004.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting gemma2 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF into f16 GGUF format.\n",
            "The output location will be /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: Finetune_Gemma_NRE_SFT_GGUF\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00004.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.float16 --> F16, shape = {3584, 256000}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00003-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.28.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.28.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.29.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.29.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.30.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.30.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.31.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.31.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.32.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.32.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00004-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.32.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.32.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.32.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.32.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.32.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.33.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.33.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.33.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.33.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.33.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.33.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.33.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.34.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.34.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.34.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.34.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.34.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.34.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.34.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.35.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.35.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.35.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.35.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.35.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.35.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.35.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.36.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.36.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.36.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.36.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.36.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.36.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.36.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.36.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.36.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.36.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.36.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.37.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.37.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.37.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.37.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.37.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.37.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.37.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.37.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.37.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.37.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.37.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.38.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.38.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.38.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.38.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.38.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.38.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.38.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.38.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.38.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.38.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.38.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.39.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.39.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.39.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.39.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.39.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.39.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.39.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.39.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.39.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.39.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.39.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.40.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.40.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.40.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.40.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.40.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.40.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.40.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.40.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.40.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.40.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.40.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.41.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.41.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.41.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.41.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.41.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.41.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.41.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.41.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.41.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.41.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.41.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Setting special token type bos to 2\n",
            "INFO:gguf.vocab:Setting special token type eos to 1\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf: n_tensors = 464, total_size = 18.5G\n",
            "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.5G/18.5G [06:44<00:00, 45.7Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 4535 (f211d1dc)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf' to '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 464 tensors from /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 9b Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 9B\n",
            "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
            "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 42\n",
            "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
            "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  169 tensors\n",
            "llama_model_loader: - type  f16:  295 tensors\n",
            "[   1/ 464]                   output_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   2/ 464]                    token_embd.weight - [ 3584, 256000,     1,     1], type =    f16, converting to q6_K .. size =  1750.00 MiB ->   717.77 MiB\n",
            "[   3/ 464]                  blk.0.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[   4/ 464]               blk.0.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   5/ 464]             blk.0.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[   6/ 464]                  blk.0.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[   7/ 464]                  blk.0.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[   8/ 464]                blk.0.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[   9/ 464]                blk.0.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  10/ 464]                blk.0.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  11/ 464]                  blk.0.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  12/ 464]     blk.0.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  13/ 464]           blk.0.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  14/ 464]                  blk.1.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  15/ 464]               blk.1.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  16/ 464]             blk.1.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  17/ 464]                  blk.1.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  18/ 464]                  blk.1.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  19/ 464]                blk.1.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  20/ 464]                blk.1.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  21/ 464]                blk.1.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  22/ 464]                  blk.1.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  23/ 464]     blk.1.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  24/ 464]           blk.1.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  25/ 464]                  blk.2.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  26/ 464]               blk.2.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  27/ 464]             blk.2.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  28/ 464]                  blk.2.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  29/ 464]                  blk.2.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  30/ 464]                blk.2.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  31/ 464]                blk.2.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  32/ 464]                blk.2.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  33/ 464]                  blk.2.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  34/ 464]     blk.2.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  35/ 464]           blk.2.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  36/ 464]                  blk.3.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  37/ 464]               blk.3.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  38/ 464]             blk.3.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  39/ 464]                  blk.3.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  40/ 464]                  blk.3.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  41/ 464]                blk.3.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  42/ 464]                blk.3.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  43/ 464]                blk.3.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  44/ 464]                  blk.3.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  45/ 464]     blk.3.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  46/ 464]           blk.3.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  47/ 464]                  blk.4.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  48/ 464]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  49/ 464]             blk.4.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  50/ 464]                  blk.4.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  51/ 464]                  blk.4.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  52/ 464]                blk.4.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  53/ 464]                blk.4.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  54/ 464]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  55/ 464]                  blk.4.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  56/ 464]     blk.4.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  57/ 464]           blk.4.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  58/ 464]                  blk.5.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  59/ 464]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  60/ 464]             blk.5.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  61/ 464]                  blk.5.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  62/ 464]                  blk.5.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  63/ 464]                blk.5.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  64/ 464]                blk.5.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  65/ 464]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  66/ 464]                  blk.5.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  67/ 464]     blk.5.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  68/ 464]           blk.5.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  69/ 464]                  blk.6.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  70/ 464]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  71/ 464]             blk.6.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  72/ 464]                  blk.6.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  73/ 464]                  blk.6.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  74/ 464]                blk.6.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  75/ 464]                blk.6.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  76/ 464]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  77/ 464]                  blk.6.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  78/ 464]     blk.6.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  79/ 464]           blk.6.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  80/ 464]                  blk.7.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  81/ 464]               blk.7.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  82/ 464]             blk.7.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  83/ 464]                  blk.7.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  84/ 464]                  blk.7.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  85/ 464]                blk.7.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  86/ 464]                blk.7.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  87/ 464]                blk.7.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  88/ 464]                  blk.7.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  89/ 464]     blk.7.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  90/ 464]           blk.7.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  91/ 464]                  blk.8.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  92/ 464]               blk.8.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  93/ 464]             blk.8.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  94/ 464]                  blk.8.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  95/ 464]                  blk.8.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  96/ 464]                blk.8.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  97/ 464]                blk.8.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  98/ 464]                blk.8.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  99/ 464]                  blk.8.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 100/ 464]     blk.8.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 101/ 464]           blk.8.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 102/ 464]                  blk.9.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 103/ 464]               blk.9.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 104/ 464]             blk.9.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 105/ 464]                  blk.9.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 106/ 464]                  blk.9.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 107/ 464]                blk.9.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 108/ 464]                blk.9.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 109/ 464]                blk.9.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 110/ 464]                  blk.9.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 111/ 464]     blk.9.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 112/ 464]           blk.9.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 113/ 464]                 blk.10.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 114/ 464]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 115/ 464]            blk.10.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 116/ 464]                 blk.10.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 117/ 464]                 blk.10.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 118/ 464]               blk.10.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 119/ 464]               blk.10.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 120/ 464]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 121/ 464]                 blk.10.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 122/ 464]    blk.10.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 123/ 464]          blk.10.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 124/ 464]                 blk.11.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 125/ 464]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 126/ 464]            blk.11.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 127/ 464]                 blk.11.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 128/ 464]                 blk.11.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 129/ 464]               blk.11.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 130/ 464]               blk.11.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 131/ 464]               blk.11.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 132/ 464]                 blk.11.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 133/ 464]    blk.11.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 134/ 464]          blk.11.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 135/ 464]                 blk.12.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 136/ 464]              blk.12.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 137/ 464]            blk.12.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 138/ 464]                 blk.12.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 139/ 464]                 blk.12.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 140/ 464]               blk.12.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 141/ 464]               blk.12.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 142/ 464]               blk.12.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 143/ 464]                 blk.12.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 144/ 464]    blk.12.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 145/ 464]          blk.12.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 146/ 464]                 blk.13.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 147/ 464]              blk.13.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 148/ 464]            blk.13.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 149/ 464]                 blk.13.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 150/ 464]                 blk.13.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 151/ 464]               blk.13.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 152/ 464]               blk.13.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 153/ 464]               blk.13.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 154/ 464]                 blk.13.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 155/ 464]    blk.13.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 156/ 464]          blk.13.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 157/ 464]                 blk.14.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 158/ 464]              blk.14.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 159/ 464]            blk.14.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 160/ 464]                 blk.14.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 161/ 464]                 blk.14.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 162/ 464]               blk.14.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 163/ 464]               blk.14.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 164/ 464]               blk.14.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 165/ 464]                 blk.14.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 166/ 464]    blk.14.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 167/ 464]          blk.14.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 168/ 464]                 blk.15.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 169/ 464]              blk.15.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 170/ 464]            blk.15.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 171/ 464]                 blk.15.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 172/ 464]                 blk.15.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 173/ 464]               blk.15.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 174/ 464]               blk.15.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 175/ 464]               blk.15.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 176/ 464]                 blk.15.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 177/ 464]    blk.15.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 178/ 464]          blk.15.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 179/ 464]                 blk.16.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 180/ 464]              blk.16.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 181/ 464]            blk.16.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 182/ 464]                 blk.16.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 183/ 464]                 blk.16.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 184/ 464]               blk.16.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 185/ 464]               blk.16.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 186/ 464]               blk.16.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 187/ 464]                 blk.16.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 188/ 464]    blk.16.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 189/ 464]          blk.16.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 190/ 464]                 blk.17.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 191/ 464]              blk.17.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 192/ 464]            blk.17.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 193/ 464]                 blk.17.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 194/ 464]                 blk.17.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 195/ 464]               blk.17.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 196/ 464]               blk.17.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 197/ 464]               blk.17.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 198/ 464]                 blk.17.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 199/ 464]    blk.17.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 200/ 464]          blk.17.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 201/ 464]                 blk.18.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 202/ 464]              blk.18.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 203/ 464]            blk.18.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 204/ 464]                 blk.18.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 205/ 464]                 blk.18.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 206/ 464]               blk.18.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 207/ 464]               blk.18.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 208/ 464]               blk.18.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 209/ 464]                 blk.18.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 210/ 464]    blk.18.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 211/ 464]          blk.18.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 212/ 464]                 blk.19.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 213/ 464]              blk.19.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 214/ 464]            blk.19.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 215/ 464]                 blk.19.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 216/ 464]                 blk.19.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 217/ 464]               blk.19.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 218/ 464]               blk.19.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 219/ 464]               blk.19.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 220/ 464]                 blk.19.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 221/ 464]    blk.19.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 222/ 464]          blk.19.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 223/ 464]                 blk.20.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 224/ 464]              blk.20.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 225/ 464]            blk.20.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 226/ 464]                 blk.20.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 227/ 464]                 blk.20.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 228/ 464]               blk.20.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 229/ 464]               blk.20.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 230/ 464]               blk.20.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 231/ 464]                 blk.20.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 232/ 464]    blk.20.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 233/ 464]          blk.20.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 234/ 464]                 blk.21.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 235/ 464]              blk.21.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 236/ 464]            blk.21.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 237/ 464]                 blk.21.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 238/ 464]                 blk.21.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 239/ 464]               blk.21.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 240/ 464]               blk.21.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 241/ 464]               blk.21.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 242/ 464]                 blk.21.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 243/ 464]    blk.21.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 244/ 464]          blk.21.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 245/ 464]                 blk.22.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 246/ 464]              blk.22.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 247/ 464]            blk.22.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 248/ 464]                 blk.22.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 249/ 464]                 blk.22.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 250/ 464]               blk.22.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 251/ 464]               blk.22.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 252/ 464]               blk.22.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 253/ 464]                 blk.22.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 254/ 464]    blk.22.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 255/ 464]          blk.22.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 256/ 464]                 blk.23.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 257/ 464]              blk.23.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 258/ 464]            blk.23.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 259/ 464]                 blk.23.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 260/ 464]                 blk.23.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 261/ 464]               blk.23.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 262/ 464]               blk.23.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 263/ 464]               blk.23.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 264/ 464]                 blk.23.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 265/ 464]    blk.23.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 266/ 464]          blk.23.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 267/ 464]                 blk.24.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 268/ 464]              blk.24.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 269/ 464]            blk.24.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 270/ 464]                 blk.24.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 271/ 464]                 blk.24.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 272/ 464]               blk.24.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 273/ 464]               blk.24.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 274/ 464]               blk.24.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 275/ 464]                 blk.24.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 276/ 464]    blk.24.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 277/ 464]          blk.24.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 278/ 464]                 blk.25.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 279/ 464]              blk.25.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 280/ 464]            blk.25.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 281/ 464]                 blk.25.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 282/ 464]                 blk.25.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 283/ 464]               blk.25.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 284/ 464]               blk.25.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 285/ 464]               blk.25.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 286/ 464]                 blk.25.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 287/ 464]    blk.25.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 288/ 464]          blk.25.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 289/ 464]                 blk.26.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 290/ 464]              blk.26.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 291/ 464]            blk.26.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 292/ 464]                 blk.26.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 293/ 464]                 blk.26.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 294/ 464]               blk.26.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 295/ 464]               blk.26.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 296/ 464]               blk.26.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 297/ 464]                 blk.26.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 298/ 464]    blk.26.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 299/ 464]          blk.26.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 300/ 464]                 blk.27.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 301/ 464]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 302/ 464]            blk.27.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 303/ 464]                 blk.27.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 304/ 464]                 blk.27.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 305/ 464]               blk.27.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 306/ 464]               blk.27.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 307/ 464]               blk.27.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 308/ 464]                 blk.27.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 309/ 464]    blk.27.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 310/ 464]          blk.27.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 311/ 464]                 blk.28.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 312/ 464]              blk.28.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 313/ 464]            blk.28.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 314/ 464]                 blk.28.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 315/ 464]                 blk.28.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 316/ 464]               blk.28.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 317/ 464]               blk.28.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 318/ 464]               blk.28.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 319/ 464]                 blk.28.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 320/ 464]    blk.28.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 321/ 464]          blk.28.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 322/ 464]                 blk.29.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 323/ 464]              blk.29.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 324/ 464]            blk.29.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 325/ 464]                 blk.29.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 326/ 464]                 blk.29.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 327/ 464]               blk.29.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 328/ 464]               blk.29.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 329/ 464]               blk.29.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 330/ 464]                 blk.29.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 331/ 464]    blk.29.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 332/ 464]          blk.29.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 333/ 464]                 blk.30.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 334/ 464]              blk.30.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 335/ 464]            blk.30.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 336/ 464]                 blk.30.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 337/ 464]                 blk.30.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 338/ 464]               blk.30.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 339/ 464]               blk.30.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 340/ 464]               blk.30.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 341/ 464]                 blk.30.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 342/ 464]    blk.30.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 343/ 464]          blk.30.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 344/ 464]                 blk.31.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 345/ 464]              blk.31.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 346/ 464]            blk.31.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 347/ 464]                 blk.31.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 348/ 464]                 blk.31.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 349/ 464]               blk.31.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 350/ 464]               blk.31.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 351/ 464]               blk.31.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 352/ 464]                 blk.31.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 353/ 464]    blk.31.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 354/ 464]          blk.31.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 355/ 464]                 blk.32.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 356/ 464]              blk.32.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 357/ 464]            blk.32.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 358/ 464]                 blk.32.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 359/ 464]                 blk.32.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 360/ 464]               blk.32.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 361/ 464]               blk.32.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 362/ 464]               blk.32.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 363/ 464]                 blk.32.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 364/ 464]    blk.32.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 365/ 464]          blk.32.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 366/ 464]                 blk.33.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 367/ 464]              blk.33.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 368/ 464]            blk.33.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 369/ 464]                 blk.33.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 370/ 464]                 blk.33.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 371/ 464]               blk.33.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 372/ 464]               blk.33.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 373/ 464]               blk.33.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 374/ 464]                 blk.33.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 375/ 464]    blk.33.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 376/ 464]          blk.33.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 377/ 464]                 blk.34.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 378/ 464]              blk.34.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 379/ 464]            blk.34.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 380/ 464]                 blk.34.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 381/ 464]                 blk.34.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 382/ 464]               blk.34.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 383/ 464]               blk.34.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 384/ 464]               blk.34.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 385/ 464]                 blk.34.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 386/ 464]    blk.34.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 387/ 464]          blk.34.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 388/ 464]                 blk.35.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 389/ 464]              blk.35.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 390/ 464]            blk.35.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 391/ 464]                 blk.35.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 392/ 464]                 blk.35.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 393/ 464]               blk.35.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 394/ 464]               blk.35.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 395/ 464]               blk.35.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 396/ 464]                 blk.35.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 397/ 464]    blk.35.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 398/ 464]          blk.35.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 399/ 464]                 blk.36.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 400/ 464]              blk.36.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 401/ 464]            blk.36.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 402/ 464]                 blk.36.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 403/ 464]                 blk.36.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 404/ 464]               blk.36.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 405/ 464]               blk.36.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 406/ 464]               blk.36.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 407/ 464]                 blk.36.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 408/ 464]    blk.36.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 409/ 464]          blk.36.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 410/ 464]                 blk.37.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 411/ 464]              blk.37.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 412/ 464]            blk.37.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 413/ 464]                 blk.37.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 414/ 464]                 blk.37.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 415/ 464]               blk.37.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 416/ 464]               blk.37.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 417/ 464]               blk.37.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 418/ 464]                 blk.37.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 419/ 464]    blk.37.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 420/ 464]          blk.37.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 421/ 464]                 blk.38.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 422/ 464]              blk.38.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 423/ 464]            blk.38.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 424/ 464]                 blk.38.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 425/ 464]                 blk.38.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 426/ 464]               blk.38.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 427/ 464]               blk.38.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 428/ 464]               blk.38.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 429/ 464]                 blk.38.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 430/ 464]    blk.38.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 431/ 464]          blk.38.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 432/ 464]                 blk.39.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 433/ 464]              blk.39.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 434/ 464]            blk.39.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 435/ 464]                 blk.39.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 436/ 464]                 blk.39.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 437/ 464]               blk.39.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 438/ 464]               blk.39.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 439/ 464]               blk.39.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 440/ 464]                 blk.39.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 441/ 464]    blk.39.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 442/ 464]          blk.39.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 443/ 464]                 blk.40.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 444/ 464]              blk.40.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 445/ 464]            blk.40.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 446/ 464]                 blk.40.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 447/ 464]                 blk.40.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 448/ 464]               blk.40.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 449/ 464]               blk.40.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 450/ 464]               blk.40.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 451/ 464]                 blk.40.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 452/ 464]    blk.40.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 453/ 464]          blk.40.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 454/ 464]                 blk.41.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 455/ 464]              blk.41.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 456/ 464]            blk.41.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 457/ 464]                 blk.41.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 458/ 464]                 blk.41.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 459/ 464]               blk.41.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 460/ 464]               blk.41.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 461/ 464]               blk.41.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 462/ 464]                 blk.41.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 463/ 464]    blk.41.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 464/ 464]          blk.41.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "llama_model_quantize_impl: model size  = 17628.31 MB\n",
            "llama_model_quantize_impl: quant size  =  5488.40 MB\n",
            "\n",
            "main: quantize time = 1204904.48 ms\n",
            "main:    total time = 1204904.48 ms\n",
            "Unsloth: Conversion completed! Output location: /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.Q4_K_M.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q8_0. This might take 20 minutes...\n",
            "main: build = 4535 (f211d1dc)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf' to '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.Q8_0.gguf' as Q8_0 using 4 threads\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 464 tensors from /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 9b Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 9B\n",
            "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
            "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 42\n",
            "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
            "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  169 tensors\n",
            "llama_model_loader: - type  f16:  295 tensors\n",
            "[   1/ 464]                   output_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   2/ 464]                    token_embd.weight - [ 3584, 256000,     1,     1], type =    f16, converting to q8_0 .. size =  1750.00 MiB ->   929.69 MiB\n",
            "[   3/ 464]                  blk.0.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[   4/ 464]               blk.0.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   5/ 464]             blk.0.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[   6/ 464]                  blk.0.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[   7/ 464]                  blk.0.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[   8/ 464]                blk.0.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[   9/ 464]                blk.0.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  10/ 464]                blk.0.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  11/ 464]                  blk.0.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  12/ 464]     blk.0.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  13/ 464]           blk.0.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  14/ 464]                  blk.1.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  15/ 464]               blk.1.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  16/ 464]             blk.1.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  17/ 464]                  blk.1.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  18/ 464]                  blk.1.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  19/ 464]                blk.1.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  20/ 464]                blk.1.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  21/ 464]                blk.1.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  22/ 464]                  blk.1.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  23/ 464]     blk.1.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  24/ 464]           blk.1.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  25/ 464]                  blk.2.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  26/ 464]               blk.2.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  27/ 464]             blk.2.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  28/ 464]                  blk.2.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  29/ 464]                  blk.2.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  30/ 464]                blk.2.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  31/ 464]                blk.2.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  32/ 464]                blk.2.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  33/ 464]                  blk.2.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  34/ 464]     blk.2.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  35/ 464]           blk.2.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  36/ 464]                  blk.3.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  37/ 464]               blk.3.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  38/ 464]             blk.3.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  39/ 464]                  blk.3.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  40/ 464]                  blk.3.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  41/ 464]                blk.3.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  42/ 464]                blk.3.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  43/ 464]                blk.3.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  44/ 464]                  blk.3.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  45/ 464]     blk.3.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  46/ 464]           blk.3.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  47/ 464]                  blk.4.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  48/ 464]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  49/ 464]             blk.4.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  50/ 464]                  blk.4.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  51/ 464]                  blk.4.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  52/ 464]                blk.4.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  53/ 464]                blk.4.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  54/ 464]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  55/ 464]                  blk.4.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  56/ 464]     blk.4.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  57/ 464]           blk.4.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  58/ 464]                  blk.5.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  59/ 464]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  60/ 464]             blk.5.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  61/ 464]                  blk.5.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  62/ 464]                  blk.5.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  63/ 464]                blk.5.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  64/ 464]                blk.5.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  65/ 464]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  66/ 464]                  blk.5.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  67/ 464]     blk.5.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  68/ 464]           blk.5.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  69/ 464]                  blk.6.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  70/ 464]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  71/ 464]             blk.6.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  72/ 464]                  blk.6.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  73/ 464]                  blk.6.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  74/ 464]                blk.6.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  75/ 464]                blk.6.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  76/ 464]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  77/ 464]                  blk.6.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  78/ 464]     blk.6.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  79/ 464]           blk.6.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  80/ 464]                  blk.7.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  81/ 464]               blk.7.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  82/ 464]             blk.7.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  83/ 464]                  blk.7.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  84/ 464]                  blk.7.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  85/ 464]                blk.7.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  86/ 464]                blk.7.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  87/ 464]                blk.7.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  88/ 464]                  blk.7.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  89/ 464]     blk.7.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  90/ 464]           blk.7.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  91/ 464]                  blk.8.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  92/ 464]               blk.8.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  93/ 464]             blk.8.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  94/ 464]                  blk.8.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  95/ 464]                  blk.8.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  96/ 464]                blk.8.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  97/ 464]                blk.8.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  98/ 464]                blk.8.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  99/ 464]                  blk.8.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 100/ 464]     blk.8.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 101/ 464]           blk.8.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 102/ 464]                  blk.9.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 103/ 464]               blk.9.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 104/ 464]             blk.9.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 105/ 464]                  blk.9.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 106/ 464]                  blk.9.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 107/ 464]                blk.9.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 108/ 464]                blk.9.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 109/ 464]                blk.9.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 110/ 464]                  blk.9.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 111/ 464]     blk.9.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 112/ 464]           blk.9.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 113/ 464]                 blk.10.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 114/ 464]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 115/ 464]            blk.10.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 116/ 464]                 blk.10.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 117/ 464]                 blk.10.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 118/ 464]               blk.10.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 119/ 464]               blk.10.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 120/ 464]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 121/ 464]                 blk.10.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 122/ 464]    blk.10.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 123/ 464]          blk.10.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 124/ 464]                 blk.11.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 125/ 464]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 126/ 464]            blk.11.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 127/ 464]                 blk.11.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 128/ 464]                 blk.11.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 129/ 464]               blk.11.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 130/ 464]               blk.11.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 131/ 464]               blk.11.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 132/ 464]                 blk.11.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 133/ 464]    blk.11.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 134/ 464]          blk.11.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 135/ 464]                 blk.12.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 136/ 464]              blk.12.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 137/ 464]            blk.12.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 138/ 464]                 blk.12.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 139/ 464]                 blk.12.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 140/ 464]               blk.12.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 141/ 464]               blk.12.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 142/ 464]               blk.12.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 143/ 464]                 blk.12.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 144/ 464]    blk.12.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 145/ 464]          blk.12.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 146/ 464]                 blk.13.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 147/ 464]              blk.13.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 148/ 464]            blk.13.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 149/ 464]                 blk.13.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 150/ 464]                 blk.13.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 151/ 464]               blk.13.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 152/ 464]               blk.13.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 153/ 464]               blk.13.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 154/ 464]                 blk.13.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 155/ 464]    blk.13.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 156/ 464]          blk.13.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 157/ 464]                 blk.14.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 158/ 464]              blk.14.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 159/ 464]            blk.14.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 160/ 464]                 blk.14.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 161/ 464]                 blk.14.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 162/ 464]               blk.14.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 163/ 464]               blk.14.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 164/ 464]               blk.14.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 165/ 464]                 blk.14.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 166/ 464]    blk.14.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 167/ 464]          blk.14.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 168/ 464]                 blk.15.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 169/ 464]              blk.15.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 170/ 464]            blk.15.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 171/ 464]                 blk.15.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 172/ 464]                 blk.15.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 173/ 464]               blk.15.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 174/ 464]               blk.15.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 175/ 464]               blk.15.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 176/ 464]                 blk.15.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 177/ 464]    blk.15.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 178/ 464]          blk.15.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 179/ 464]                 blk.16.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 180/ 464]              blk.16.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 181/ 464]            blk.16.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 182/ 464]                 blk.16.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 183/ 464]                 blk.16.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 184/ 464]               blk.16.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 185/ 464]               blk.16.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 186/ 464]               blk.16.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 187/ 464]                 blk.16.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 188/ 464]    blk.16.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 189/ 464]          blk.16.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 190/ 464]                 blk.17.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 191/ 464]              blk.17.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 192/ 464]            blk.17.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 193/ 464]                 blk.17.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 194/ 464]                 blk.17.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 195/ 464]               blk.17.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 196/ 464]               blk.17.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 197/ 464]               blk.17.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 198/ 464]                 blk.17.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 199/ 464]    blk.17.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 200/ 464]          blk.17.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 201/ 464]                 blk.18.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 202/ 464]              blk.18.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 203/ 464]            blk.18.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 204/ 464]                 blk.18.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 205/ 464]                 blk.18.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 206/ 464]               blk.18.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 207/ 464]               blk.18.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 208/ 464]               blk.18.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 209/ 464]                 blk.18.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 210/ 464]    blk.18.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 211/ 464]          blk.18.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 212/ 464]                 blk.19.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 213/ 464]              blk.19.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 214/ 464]            blk.19.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 215/ 464]                 blk.19.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 216/ 464]                 blk.19.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 217/ 464]               blk.19.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 218/ 464]               blk.19.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 219/ 464]               blk.19.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 220/ 464]                 blk.19.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 221/ 464]    blk.19.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 222/ 464]          blk.19.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 223/ 464]                 blk.20.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 224/ 464]              blk.20.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 225/ 464]            blk.20.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 226/ 464]                 blk.20.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 227/ 464]                 blk.20.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 228/ 464]               blk.20.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 229/ 464]               blk.20.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 230/ 464]               blk.20.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 231/ 464]                 blk.20.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 232/ 464]    blk.20.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 233/ 464]          blk.20.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 234/ 464]                 blk.21.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 235/ 464]              blk.21.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 236/ 464]            blk.21.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 237/ 464]                 blk.21.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 238/ 464]                 blk.21.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 239/ 464]               blk.21.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 240/ 464]               blk.21.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 241/ 464]               blk.21.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 242/ 464]                 blk.21.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 243/ 464]    blk.21.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 244/ 464]          blk.21.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 245/ 464]                 blk.22.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 246/ 464]              blk.22.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 247/ 464]            blk.22.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 248/ 464]                 blk.22.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 249/ 464]                 blk.22.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 250/ 464]               blk.22.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 251/ 464]               blk.22.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 252/ 464]               blk.22.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 253/ 464]                 blk.22.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 254/ 464]    blk.22.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 255/ 464]          blk.22.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 256/ 464]                 blk.23.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 257/ 464]              blk.23.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 258/ 464]            blk.23.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 259/ 464]                 blk.23.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 260/ 464]                 blk.23.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 261/ 464]               blk.23.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 262/ 464]               blk.23.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 263/ 464]               blk.23.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 264/ 464]                 blk.23.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 265/ 464]    blk.23.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 266/ 464]          blk.23.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 267/ 464]                 blk.24.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 268/ 464]              blk.24.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 269/ 464]            blk.24.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 270/ 464]                 blk.24.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 271/ 464]                 blk.24.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 272/ 464]               blk.24.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 273/ 464]               blk.24.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 274/ 464]               blk.24.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 275/ 464]                 blk.24.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 276/ 464]    blk.24.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 277/ 464]          blk.24.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 278/ 464]                 blk.25.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 279/ 464]              blk.25.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 280/ 464]            blk.25.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 281/ 464]                 blk.25.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 282/ 464]                 blk.25.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 283/ 464]               blk.25.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 284/ 464]               blk.25.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 285/ 464]               blk.25.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 286/ 464]                 blk.25.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 287/ 464]    blk.25.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 288/ 464]          blk.25.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 289/ 464]                 blk.26.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 290/ 464]              blk.26.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 291/ 464]            blk.26.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 292/ 464]                 blk.26.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 293/ 464]                 blk.26.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 294/ 464]               blk.26.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 295/ 464]               blk.26.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 296/ 464]               blk.26.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 297/ 464]                 blk.26.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 298/ 464]    blk.26.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 299/ 464]          blk.26.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 300/ 464]                 blk.27.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 301/ 464]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 302/ 464]            blk.27.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 303/ 464]                 blk.27.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 304/ 464]                 blk.27.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 305/ 464]               blk.27.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 306/ 464]               blk.27.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 307/ 464]               blk.27.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 308/ 464]                 blk.27.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 309/ 464]    blk.27.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 310/ 464]          blk.27.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 311/ 464]                 blk.28.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 312/ 464]              blk.28.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 313/ 464]            blk.28.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 314/ 464]                 blk.28.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 315/ 464]                 blk.28.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 316/ 464]               blk.28.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 317/ 464]               blk.28.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 318/ 464]               blk.28.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 319/ 464]                 blk.28.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 320/ 464]    blk.28.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 321/ 464]          blk.28.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 322/ 464]                 blk.29.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 323/ 464]              blk.29.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 324/ 464]            blk.29.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 325/ 464]                 blk.29.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 326/ 464]                 blk.29.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 327/ 464]               blk.29.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 328/ 464]               blk.29.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 329/ 464]               blk.29.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 330/ 464]                 blk.29.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 331/ 464]    blk.29.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 332/ 464]          blk.29.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 333/ 464]                 blk.30.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 334/ 464]              blk.30.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 335/ 464]            blk.30.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 336/ 464]                 blk.30.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 337/ 464]                 blk.30.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 338/ 464]               blk.30.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 339/ 464]               blk.30.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 340/ 464]               blk.30.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 341/ 464]                 blk.30.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 342/ 464]    blk.30.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 343/ 464]          blk.30.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 344/ 464]                 blk.31.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 345/ 464]              blk.31.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 346/ 464]            blk.31.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 347/ 464]                 blk.31.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 348/ 464]                 blk.31.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 349/ 464]               blk.31.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 350/ 464]               blk.31.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 351/ 464]               blk.31.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 352/ 464]                 blk.31.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 353/ 464]    blk.31.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 354/ 464]          blk.31.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 355/ 464]                 blk.32.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 356/ 464]              blk.32.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 357/ 464]            blk.32.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 358/ 464]                 blk.32.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 359/ 464]                 blk.32.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 360/ 464]               blk.32.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 361/ 464]               blk.32.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 362/ 464]               blk.32.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 363/ 464]                 blk.32.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 364/ 464]    blk.32.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 365/ 464]          blk.32.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 366/ 464]                 blk.33.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 367/ 464]              blk.33.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 368/ 464]            blk.33.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 369/ 464]                 blk.33.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 370/ 464]                 blk.33.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 371/ 464]               blk.33.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 372/ 464]               blk.33.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 373/ 464]               blk.33.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 374/ 464]                 blk.33.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 375/ 464]    blk.33.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 376/ 464]          blk.33.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 377/ 464]                 blk.34.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 378/ 464]              blk.34.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 379/ 464]            blk.34.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 380/ 464]                 blk.34.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 381/ 464]                 blk.34.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 382/ 464]               blk.34.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 383/ 464]               blk.34.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 384/ 464]               blk.34.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 385/ 464]                 blk.34.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 386/ 464]    blk.34.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 387/ 464]          blk.34.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 388/ 464]                 blk.35.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 389/ 464]              blk.35.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 390/ 464]            blk.35.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 391/ 464]                 blk.35.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 392/ 464]                 blk.35.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 393/ 464]               blk.35.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 394/ 464]               blk.35.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 395/ 464]               blk.35.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 396/ 464]                 blk.35.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 397/ 464]    blk.35.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 398/ 464]          blk.35.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 399/ 464]                 blk.36.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 400/ 464]              blk.36.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 401/ 464]            blk.36.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 402/ 464]                 blk.36.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 403/ 464]                 blk.36.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 404/ 464]               blk.36.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 405/ 464]               blk.36.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 406/ 464]               blk.36.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 407/ 464]                 blk.36.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 408/ 464]    blk.36.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 409/ 464]          blk.36.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 410/ 464]                 blk.37.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 411/ 464]              blk.37.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 412/ 464]            blk.37.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 413/ 464]                 blk.37.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 414/ 464]                 blk.37.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 415/ 464]               blk.37.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 416/ 464]               blk.37.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 417/ 464]               blk.37.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 418/ 464]                 blk.37.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 419/ 464]    blk.37.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 420/ 464]          blk.37.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 421/ 464]                 blk.38.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 422/ 464]              blk.38.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 423/ 464]            blk.38.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 424/ 464]                 blk.38.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 425/ 464]                 blk.38.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 426/ 464]               blk.38.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 427/ 464]               blk.38.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 428/ 464]               blk.38.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 429/ 464]                 blk.38.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 430/ 464]    blk.38.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 431/ 464]          blk.38.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 432/ 464]                 blk.39.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 433/ 464]              blk.39.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 434/ 464]            blk.39.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 435/ 464]                 blk.39.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 436/ 464]                 blk.39.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 437/ 464]               blk.39.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 438/ 464]               blk.39.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 439/ 464]               blk.39.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 440/ 464]                 blk.39.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 441/ 464]    blk.39.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 442/ 464]          blk.39.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 443/ 464]                 blk.40.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 444/ 464]              blk.40.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 445/ 464]            blk.40.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 446/ 464]                 blk.40.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 447/ 464]                 blk.40.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 448/ 464]               blk.40.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 449/ 464]               blk.40.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 450/ 464]               blk.40.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 451/ 464]                 blk.40.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 452/ 464]    blk.40.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 453/ 464]          blk.40.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 454/ 464]                 blk.41.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 455/ 464]              blk.41.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 456/ 464]            blk.41.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 457/ 464]                 blk.41.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 458/ 464]                 blk.41.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 459/ 464]               blk.41.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 460/ 464]               blk.41.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 461/ 464]               blk.41.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 462/ 464]                 blk.41.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 463/ 464]    blk.41.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 464/ 464]          blk.41.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "llama_model_quantize_impl: model size  = 17628.31 MB\n",
            "llama_model_quantize_impl: quant size  =  9366.12 MB\n",
            "\n",
            "main: quantize time = 1349406.95 ms\n",
            "main:    total time = 1349406.95 ms\n",
            "Unsloth: Conversion completed! Output location: /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.Q8_0.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This might take 20 minutes...\n",
            "main: build = 4535 (f211d1dc)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf' to '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.Q5_K_M.gguf' as Q5_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 464 tensors from /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 9b Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 9B\n",
            "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
            "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 42\n",
            "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
            "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  169 tensors\n",
            "llama_model_loader: - type  f16:  295 tensors\n",
            "[   1/ 464]                   output_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   2/ 464]                    token_embd.weight - [ 3584, 256000,     1,     1], type =    f16, converting to q6_K .. size =  1750.00 MiB ->   717.77 MiB\n",
            "[   3/ 464]                  blk.0.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[   4/ 464]               blk.0.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   5/ 464]             blk.0.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[   6/ 464]                  blk.0.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[   7/ 464]                  blk.0.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[   8/ 464]                blk.0.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[   9/ 464]                blk.0.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  10/ 464]                blk.0.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  11/ 464]                  blk.0.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  12/ 464]     blk.0.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  13/ 464]           blk.0.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  14/ 464]                  blk.1.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  15/ 464]               blk.1.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  16/ 464]             blk.1.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  17/ 464]                  blk.1.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  18/ 464]                  blk.1.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  19/ 464]                blk.1.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  20/ 464]                blk.1.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  21/ 464]                blk.1.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  22/ 464]                  blk.1.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  23/ 464]     blk.1.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  24/ 464]           blk.1.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  25/ 464]                  blk.2.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  26/ 464]               blk.2.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  27/ 464]             blk.2.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  28/ 464]                  blk.2.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  29/ 464]                  blk.2.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  30/ 464]                blk.2.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  31/ 464]                blk.2.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  32/ 464]                blk.2.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  33/ 464]                  blk.2.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  34/ 464]     blk.2.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  35/ 464]           blk.2.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  36/ 464]                  blk.3.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  37/ 464]               blk.3.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  38/ 464]             blk.3.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  39/ 464]                  blk.3.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  40/ 464]                  blk.3.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  41/ 464]                blk.3.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  42/ 464]                blk.3.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  43/ 464]                blk.3.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  44/ 464]                  blk.3.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  45/ 464]     blk.3.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  46/ 464]           blk.3.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  47/ 464]                  blk.4.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  48/ 464]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  49/ 464]             blk.4.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  50/ 464]                  blk.4.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  51/ 464]                  blk.4.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  52/ 464]                blk.4.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  53/ 464]                blk.4.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  54/ 464]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  55/ 464]                  blk.4.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  56/ 464]     blk.4.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  57/ 464]           blk.4.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  58/ 464]                  blk.5.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  59/ 464]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  60/ 464]             blk.5.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  61/ 464]                  blk.5.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  62/ 464]                  blk.5.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  63/ 464]                blk.5.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  64/ 464]                blk.5.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  65/ 464]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  66/ 464]                  blk.5.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  67/ 464]     blk.5.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  68/ 464]           blk.5.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  69/ 464]                  blk.6.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  70/ 464]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  71/ 464]             blk.6.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  72/ 464]                  blk.6.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  73/ 464]                  blk.6.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  74/ 464]                blk.6.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  75/ 464]                blk.6.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  76/ 464]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  77/ 464]                  blk.6.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  78/ 464]     blk.6.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  79/ 464]           blk.6.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  80/ 464]                  blk.7.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  81/ 464]               blk.7.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  82/ 464]             blk.7.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  83/ 464]                  blk.7.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  84/ 464]                  blk.7.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  85/ 464]                blk.7.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  86/ 464]                blk.7.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  87/ 464]                blk.7.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  88/ 464]                  blk.7.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  89/ 464]     blk.7.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  90/ 464]           blk.7.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  91/ 464]                  blk.8.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  92/ 464]               blk.8.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  93/ 464]             blk.8.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  94/ 464]                  blk.8.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[  95/ 464]                  blk.8.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[  96/ 464]                blk.8.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  97/ 464]                blk.8.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[  98/ 464]                blk.8.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  99/ 464]                  blk.8.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 100/ 464]     blk.8.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 101/ 464]           blk.8.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 102/ 464]                  blk.9.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 103/ 464]               blk.9.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 104/ 464]             blk.9.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 105/ 464]                  blk.9.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 106/ 464]                  blk.9.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 107/ 464]                blk.9.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 108/ 464]                blk.9.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 109/ 464]                blk.9.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 110/ 464]                  blk.9.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 111/ 464]     blk.9.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 112/ 464]           blk.9.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 113/ 464]                 blk.10.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 114/ 464]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 115/ 464]            blk.10.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 116/ 464]                 blk.10.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 117/ 464]                 blk.10.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 118/ 464]               blk.10.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 119/ 464]               blk.10.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 120/ 464]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 121/ 464]                 blk.10.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 122/ 464]    blk.10.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 123/ 464]          blk.10.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 124/ 464]                 blk.11.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 125/ 464]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 126/ 464]            blk.11.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 127/ 464]                 blk.11.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 128/ 464]                 blk.11.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 129/ 464]               blk.11.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 130/ 464]               blk.11.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 131/ 464]               blk.11.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 132/ 464]                 blk.11.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 133/ 464]    blk.11.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 134/ 464]          blk.11.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 135/ 464]                 blk.12.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 136/ 464]              blk.12.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 137/ 464]            blk.12.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 138/ 464]                 blk.12.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 139/ 464]                 blk.12.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 140/ 464]               blk.12.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 141/ 464]               blk.12.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 142/ 464]               blk.12.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 143/ 464]                 blk.12.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 144/ 464]    blk.12.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 145/ 464]          blk.12.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 146/ 464]                 blk.13.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 147/ 464]              blk.13.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 148/ 464]            blk.13.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 149/ 464]                 blk.13.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 150/ 464]                 blk.13.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 151/ 464]               blk.13.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 152/ 464]               blk.13.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 153/ 464]               blk.13.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 154/ 464]                 blk.13.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 155/ 464]    blk.13.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 156/ 464]          blk.13.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 157/ 464]                 blk.14.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 158/ 464]              blk.14.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 159/ 464]            blk.14.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 160/ 464]                 blk.14.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 161/ 464]                 blk.14.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 162/ 464]               blk.14.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 163/ 464]               blk.14.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 164/ 464]               blk.14.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 165/ 464]                 blk.14.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 166/ 464]    blk.14.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 167/ 464]          blk.14.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 168/ 464]                 blk.15.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 169/ 464]              blk.15.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 170/ 464]            blk.15.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 171/ 464]                 blk.15.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 172/ 464]                 blk.15.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 173/ 464]               blk.15.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 174/ 464]               blk.15.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 175/ 464]               blk.15.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 176/ 464]                 blk.15.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 177/ 464]    blk.15.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 178/ 464]          blk.15.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 179/ 464]                 blk.16.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 180/ 464]              blk.16.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 181/ 464]            blk.16.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 182/ 464]                 blk.16.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 183/ 464]                 blk.16.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 184/ 464]               blk.16.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 185/ 464]               blk.16.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 186/ 464]               blk.16.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 187/ 464]                 blk.16.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 188/ 464]    blk.16.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 189/ 464]          blk.16.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 190/ 464]                 blk.17.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 191/ 464]              blk.17.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 192/ 464]            blk.17.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 193/ 464]                 blk.17.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 194/ 464]                 blk.17.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 195/ 464]               blk.17.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 196/ 464]               blk.17.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 197/ 464]               blk.17.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 198/ 464]                 blk.17.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 199/ 464]    blk.17.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 200/ 464]          blk.17.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 201/ 464]                 blk.18.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 202/ 464]              blk.18.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 203/ 464]            blk.18.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 204/ 464]                 blk.18.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 205/ 464]                 blk.18.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 206/ 464]               blk.18.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 207/ 464]               blk.18.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 208/ 464]               blk.18.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 209/ 464]                 blk.18.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 210/ 464]    blk.18.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 211/ 464]          blk.18.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 212/ 464]                 blk.19.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 213/ 464]              blk.19.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 214/ 464]            blk.19.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 215/ 464]                 blk.19.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 216/ 464]                 blk.19.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 217/ 464]               blk.19.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 218/ 464]               blk.19.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 219/ 464]               blk.19.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 220/ 464]                 blk.19.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 221/ 464]    blk.19.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 222/ 464]          blk.19.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 223/ 464]                 blk.20.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 224/ 464]              blk.20.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 225/ 464]            blk.20.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 226/ 464]                 blk.20.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 227/ 464]                 blk.20.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 228/ 464]               blk.20.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 229/ 464]               blk.20.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 230/ 464]               blk.20.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 231/ 464]                 blk.20.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 232/ 464]    blk.20.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 233/ 464]          blk.20.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 234/ 464]                 blk.21.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 235/ 464]              blk.21.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 236/ 464]            blk.21.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 237/ 464]                 blk.21.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 238/ 464]                 blk.21.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 239/ 464]               blk.21.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 240/ 464]               blk.21.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 241/ 464]               blk.21.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 242/ 464]                 blk.21.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 243/ 464]    blk.21.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 244/ 464]          blk.21.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 245/ 464]                 blk.22.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 246/ 464]              blk.22.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 247/ 464]            blk.22.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 248/ 464]                 blk.22.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 249/ 464]                 blk.22.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 250/ 464]               blk.22.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 251/ 464]               blk.22.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 252/ 464]               blk.22.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 253/ 464]                 blk.22.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 254/ 464]    blk.22.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 255/ 464]          blk.22.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 256/ 464]                 blk.23.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 257/ 464]              blk.23.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 258/ 464]            blk.23.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 259/ 464]                 blk.23.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 260/ 464]                 blk.23.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 261/ 464]               blk.23.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 262/ 464]               blk.23.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 263/ 464]               blk.23.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 264/ 464]                 blk.23.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 265/ 464]    blk.23.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 266/ 464]          blk.23.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 267/ 464]                 blk.24.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 268/ 464]              blk.24.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 269/ 464]            blk.24.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 270/ 464]                 blk.24.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 271/ 464]                 blk.24.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 272/ 464]               blk.24.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 273/ 464]               blk.24.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 274/ 464]               blk.24.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 275/ 464]                 blk.24.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 276/ 464]    blk.24.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 277/ 464]          blk.24.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 278/ 464]                 blk.25.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 279/ 464]              blk.25.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 280/ 464]            blk.25.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 281/ 464]                 blk.25.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 282/ 464]                 blk.25.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 283/ 464]               blk.25.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 284/ 464]               blk.25.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 285/ 464]               blk.25.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 286/ 464]                 blk.25.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 287/ 464]    blk.25.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 288/ 464]          blk.25.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 289/ 464]                 blk.26.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 290/ 464]              blk.26.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 291/ 464]            blk.26.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 292/ 464]                 blk.26.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 293/ 464]                 blk.26.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 294/ 464]               blk.26.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 295/ 464]               blk.26.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 296/ 464]               blk.26.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 297/ 464]                 blk.26.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 298/ 464]    blk.26.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 299/ 464]          blk.26.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 300/ 464]                 blk.27.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 301/ 464]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 302/ 464]            blk.27.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 303/ 464]                 blk.27.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 304/ 464]                 blk.27.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 305/ 464]               blk.27.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 306/ 464]               blk.27.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 307/ 464]               blk.27.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 308/ 464]                 blk.27.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 309/ 464]    blk.27.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 310/ 464]          blk.27.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 311/ 464]                 blk.28.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 312/ 464]              blk.28.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 313/ 464]            blk.28.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 314/ 464]                 blk.28.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 315/ 464]                 blk.28.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 316/ 464]               blk.28.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 317/ 464]               blk.28.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 318/ 464]               blk.28.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 319/ 464]                 blk.28.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 320/ 464]    blk.28.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 321/ 464]          blk.28.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 322/ 464]                 blk.29.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 323/ 464]              blk.29.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 324/ 464]            blk.29.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 325/ 464]                 blk.29.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 326/ 464]                 blk.29.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 327/ 464]               blk.29.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 328/ 464]               blk.29.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 329/ 464]               blk.29.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 330/ 464]                 blk.29.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 331/ 464]    blk.29.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 332/ 464]          blk.29.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 333/ 464]                 blk.30.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 334/ 464]              blk.30.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 335/ 464]            blk.30.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 336/ 464]                 blk.30.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 337/ 464]                 blk.30.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 338/ 464]               blk.30.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 339/ 464]               blk.30.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 340/ 464]               blk.30.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 341/ 464]                 blk.30.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 342/ 464]    blk.30.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 343/ 464]          blk.30.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 344/ 464]                 blk.31.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 345/ 464]              blk.31.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 346/ 464]            blk.31.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 347/ 464]                 blk.31.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 348/ 464]                 blk.31.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 349/ 464]               blk.31.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 350/ 464]               blk.31.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 351/ 464]               blk.31.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 352/ 464]                 blk.31.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 353/ 464]    blk.31.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 354/ 464]          blk.31.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 355/ 464]                 blk.32.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 356/ 464]              blk.32.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 357/ 464]            blk.32.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 358/ 464]                 blk.32.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 359/ 464]                 blk.32.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 360/ 464]               blk.32.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 361/ 464]               blk.32.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 362/ 464]               blk.32.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 363/ 464]                 blk.32.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 364/ 464]    blk.32.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 365/ 464]          blk.32.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 366/ 464]                 blk.33.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 367/ 464]              blk.33.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 368/ 464]            blk.33.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 369/ 464]                 blk.33.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 370/ 464]                 blk.33.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 371/ 464]               blk.33.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 372/ 464]               blk.33.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 373/ 464]               blk.33.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 374/ 464]                 blk.33.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 375/ 464]    blk.33.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 376/ 464]          blk.33.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 377/ 464]                 blk.34.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 378/ 464]              blk.34.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 379/ 464]            blk.34.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 380/ 464]                 blk.34.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 381/ 464]                 blk.34.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 382/ 464]               blk.34.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 383/ 464]               blk.34.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 384/ 464]               blk.34.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 385/ 464]                 blk.34.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 386/ 464]    blk.34.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 387/ 464]          blk.34.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 388/ 464]                 blk.35.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 389/ 464]              blk.35.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 390/ 464]            blk.35.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 391/ 464]                 blk.35.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 392/ 464]                 blk.35.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 393/ 464]               blk.35.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 394/ 464]               blk.35.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 395/ 464]               blk.35.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 396/ 464]                 blk.35.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 397/ 464]    blk.35.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 398/ 464]          blk.35.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 399/ 464]                 blk.36.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 400/ 464]              blk.36.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 401/ 464]            blk.36.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 402/ 464]                 blk.36.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 403/ 464]                 blk.36.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 404/ 464]               blk.36.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 405/ 464]               blk.36.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 406/ 464]               blk.36.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 407/ 464]                 blk.36.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 408/ 464]    blk.36.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 409/ 464]          blk.36.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 410/ 464]                 blk.37.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 411/ 464]              blk.37.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 412/ 464]            blk.37.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 413/ 464]                 blk.37.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 414/ 464]                 blk.37.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 415/ 464]               blk.37.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 416/ 464]               blk.37.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 417/ 464]               blk.37.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 418/ 464]                 blk.37.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 419/ 464]    blk.37.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 420/ 464]          blk.37.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 421/ 464]                 blk.38.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 422/ 464]              blk.38.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 423/ 464]            blk.38.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 424/ 464]                 blk.38.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 425/ 464]                 blk.38.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 426/ 464]               blk.38.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 427/ 464]               blk.38.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 428/ 464]               blk.38.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 429/ 464]                 blk.38.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 430/ 464]    blk.38.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 431/ 464]          blk.38.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 432/ 464]                 blk.39.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 433/ 464]              blk.39.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 434/ 464]            blk.39.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 435/ 464]                 blk.39.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 436/ 464]                 blk.39.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 437/ 464]               blk.39.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 438/ 464]               blk.39.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 439/ 464]               blk.39.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 440/ 464]                 blk.39.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 441/ 464]    blk.39.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 442/ 464]          blk.39.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 443/ 464]                 blk.40.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 444/ 464]              blk.40.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 445/ 464]            blk.40.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 446/ 464]                 blk.40.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 447/ 464]                 blk.40.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 448/ 464]               blk.40.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 449/ 464]               blk.40.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 450/ 464]               blk.40.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 451/ 464]                 blk.40.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 452/ 464]    blk.40.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 453/ 464]          blk.40.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 454/ 464]                 blk.41.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\n",
            "[ 455/ 464]              blk.41.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 456/ 464]            blk.41.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 457/ 464]                 blk.41.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\n",
            "[ 458/ 464]                 blk.41.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 459/ 464]               blk.41.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 460/ 464]               blk.41.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 461/ 464]               blk.41.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 462/ 464]                 blk.41.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\n",
            "[ 463/ 464]    blk.41.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 464/ 464]          blk.41.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "llama_model_quantize_impl: model size  = 17628.31 MB\n",
            "llama_model_quantize_impl: quant size  =  6333.65 MB\n",
            "\n",
            "main: quantize time = 1380881.83 ms\n",
            "main:    total time = 1380881.83 ms\n",
            "Unsloth: Conversion completed! Output location: /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.Q5_K_M.gguf\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1a83d9c10f54e28b318e05f575f2708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "975ce87429314387b4fca236ebcfca61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.Q4_K_M.gguf:   0%|          | 0.00/5.76G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af6a61f055474681b43bfba5e6cdd21d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1aa0730f9d543d6bcc46944010bf38f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.Q8_0.gguf:   0%|          | 0.00/9.83G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        }
      ],
      "source": [
        "# Push the trained model to the Hugging Face Model Hub using the GGUF format\n",
        "model.push_to_hub_gguf(\n",
        "    \"SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF\",  # Specify the model repository path on Hugging Face Hub. Replace \"hf\" with your Hugging Face username.\n",
        "    tokenizer,  # Pass the tokenizer associated with the model to ensure compatibility on the hub\n",
        "    quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],  # Specify the quantization methods to apply for optimized model storage (e.g., q4_k_m, q8_0, q5_k_m)\n",
        "    token=\"\",  # Provide the Hugging Face token for authentication. Obtain a token at https://huggingface.co/settings/tokens\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f2d0aadcc34dbaa6db45126e7f0a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03c68aaa143145788d61d1583b5e302f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "041ef4cee1c8404389d65eddcc1f0a79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "045eed819ff4464e8c120a9edd46681d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063f95215ce24d71b297f4c4360e6225": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "085150c68013475aa4f9cc1a2659b4f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0956649daa4e4a33a0637955ceb3af4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b07674343c434449a129e150c7ea6a30",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_82b7da1cf0714fcf91f48fd402cf6831",
            "value": "â€‡46.4k/46.4kâ€‡[00:00&lt;00:00,â€‡2.87MB/s]"
          }
        },
        "0bb68ef07ba14a41b2a8c25d47458586": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c7d317541bd40b7923f7543e44df9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d41e65fe3624c57a4c42bcdb4e99a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f4098aa9dec44f08ed235b1502849b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29c63312ded547a190e166c0a256e35e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3b6afd0d1b24406bb7e5ebd234d624c",
            "value": 1
          }
        },
        "103c8c86485042798569851ef45e6b10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10a7c264c6544f35962ef351bdea8a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10fdbcdcca73414da52f6db03054aece": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13c0e7f5b76d438185e8e99ab3855a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_103c8c86485042798569851ef45e6b10",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fa99e8d6359a4e97a37a27e538b9a7c8",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "13fbcabf3fe84cecb8c7cc9e1f49fe98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14b3cd3a7f2f4770b15cf4b044bc139c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152d2743aa7747548871d293ebae1bb7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159b71d1e4814e5da5c3aa382ad105ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72d4f8be0ff54355a7cec30cf9be09fc",
            "max": 9827148032,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4d11098ccc049558178fea91da32637",
            "value": 9827148032
          }
        },
        "1ab4050223e046eeae80de31713edfb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1afd013244ba4d0288027e175de5b374": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f8a28665b7d4b049e61b3c54ba90e73",
            "max": 11989,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87965ed43d0049abbf8487fb4044499c",
            "value": 11989
          }
        },
        "1b45412bbd664b37b2c5db628c74e9e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1e305724c21436eb29f8d3adab15b43",
            "max": 46405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ea8a5a7dc7d4468b8dcef1dbc00dc2b",
            "value": 46405
          }
        },
        "1e1eee19e4124d1eb95a6a83e7e596c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "207341d8b1104761b4da223d4cafbf4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c4aa3cd39b43899605f73dcd2d1459",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ca0e08570b784c878e49c141239a7089",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "211dba39ce344a83a77f936203b56498": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eade1693f3948e2a5d74d7bd09995f2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_03c68aaa143145788d61d1583b5e302f",
            "value": "â€‡1/1â€‡[00:51&lt;00:00,â€‡51.39s/it]"
          }
        },
        "23830c112a784c429c087b7615558a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88e5ea8ea7e344609418034a6337a44b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bb578bc7bba7408a9c022ef411edb0d0",
            "value": "README.md:â€‡100%"
          }
        },
        "250e03adeaff4799b76616c5d626b900": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "268d6b2a547542a7a5daf679c4f40f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ee0f8c8739433b87383d27def8188d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2703a6d091f147ebaf3ba55e90939dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "281a8a9436cd452788ab5e7c87d46aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c63312ded547a190e166c0a256e35e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c0e825947f74d2fb4a55ba0af11664e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ea78ce8d6ce43cb8ef96319fd11eb9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eade1693f3948e2a5d74d7bd09995f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "329935d1a4d74bad95c68c8bfa72d0a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "352a9b4eef5b414787eb87b8b1983c57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "399b396ee7fb43b096133dc75d4b9df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40bcdc93adb74c4baea2e1bdda524362",
              "IPY_MODEL_abd40393f5354cbeb2c53e33c56d87c6",
              "IPY_MODEL_eb3b328104954285bf694de63e6c0d35"
            ],
            "layout": "IPY_MODEL_fa4a36ca0dc84195add7474f718b3620"
          }
        },
        "39d6be827aa64d6e9b1e3d7d55033333": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2001049d094d21b777d822fd67eb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc42a293ae70496a873b9067756520fa",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d57ce7bad734fecbde8468d93890c66",
            "value": 636
          }
        },
        "3d57ce7bad734fecbde8468d93890c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d6f676289e64df7b5755600522273ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db4d663c42a249f4b4146ae1c72fdf7b",
              "IPY_MODEL_4da48ce973fa41218122e6290918752e",
              "IPY_MODEL_5ea382ab0f684e7eaede06db9ab6dcc5"
            ],
            "layout": "IPY_MODEL_488ef5bdf49a49ad920e7874ce8bbf5d"
          }
        },
        "3f423b836fa54e229fd971bc1b91d7cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f47f0a94d184993a2f85a7c9ca44271": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40bcdc93adb74c4baea2e1bdda524362": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_281a8a9436cd452788ab5e7c87d46aa7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_13fbcabf3fe84cecb8c7cc9e1f49fe98",
            "value": "train-00000-of-00001.parquet:â€‡100%"
          }
        },
        "40da6317ee3a4419b893ab20ed61b1a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "415d5bd0b30f4c3daea62bec20e8242f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4301e4c73e964e35bb836df27a476141": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "448a7a04fa6c47af8952cab7ede820bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13c0e7f5b76d438185e8e99ab3855a3c",
              "IPY_MODEL_f784f27dd5554d4ab36f533e6251d259",
              "IPY_MODEL_add1218f050c4b87bd3c836e564e136a"
            ],
            "layout": "IPY_MODEL_045eed819ff4464e8c120a9edd46681d"
          }
        },
        "488ef5bdf49a49ad920e7874ce8bbf5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1a5f524805476d98be6a20c827ffce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bdc189a1e9f4397b07371e067b9a179": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c4b3295658e49e3b3907ef93d02d76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c9239698b2b4a6ebca119a9be635b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a7c264c6544f35962ef351bdea8a2a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8384043d99874338b3470d281ad11188",
            "value": "Mapâ€‡(num_proc=2):â€‡100%"
          }
        },
        "4da48ce973fa41218122e6290918752e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f6f928a1ced44e6ab1356f99ba36558",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_708be012414b4279a2fedfd39a03acac",
            "value": 190
          }
        },
        "503adcd44b9948c7ba9a47345d08b629": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5082df8250f2490793ae4ef2f199da19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5135a507e07744eea8549164aae45c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56dbb978bc054c74b87b8a7cfcc01627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97dae5b1902d4d3ba163e60094e14cfd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0c7d317541bd40b7923f7543e44df9c2",
            "value": "â€‡6.13G/6.13Gâ€‡[00:47&lt;00:00,â€‡414MB/s]"
          }
        },
        "58fc1e95a1674f54b055343605260948": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8723223377b4d09a00e55d06f3f703c",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1a7410a679d47149e7ff6e5a3c7c453",
            "value": 571
          }
        },
        "5bb4b8ce480b4197ba85ffc6c21f629b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e43b7ae820b47ccbeb275fff80876a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bb4b8ce480b4197ba85ffc6c21f629b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b35fb25485304c46a32536c9d7b76be8",
            "value": "â€‡4.24M/4.24Mâ€‡[00:00&lt;00:00,â€‡38.0MB/s]"
          }
        },
        "5ea382ab0f684e7eaede06db9ab6dcc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e74859d955144b3e975991245ebcbb51",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_93c65e600a164e13abcf0f34abe817a1",
            "value": "â€‡190/190â€‡[00:00&lt;00:00,â€‡13.1kB/s]"
          }
        },
        "5ea8a5a7dc7d4468b8dcef1dbc00dc2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6170800399c248c3b6c83ada8754bd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61cbb48762854c72ad83cc906139b596": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "632d465ebf1c4876975641eb1a2ca45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_207341d8b1104761b4da223d4cafbf4d",
              "IPY_MODEL_c21ca3aa13414ffeb39e68878e9e3ab0",
              "IPY_MODEL_bd8a75aa002e499f99092e0c1b5f57f6"
            ],
            "layout": "IPY_MODEL_61cbb48762854c72ad83cc906139b596"
          }
        },
        "6538fc08435e449694e110e94fd760c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ea78ce8d6ce43cb8ef96319fd11eb9f",
            "max": 5761057024,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76c59403e82447ecb393aac16d665158",
            "value": 5761057024
          }
        },
        "6bebe9bf15d84ca6a2ca01e869449c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d362fbf445045389a19c301caaa0d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8476aeb8c37842f98033e718d6e49898",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bb7249b8b40b489b934c31638afb166a",
            "value": "â€‡636/636â€‡[00:00&lt;00:00,â€‡39.8kB/s]"
          }
        },
        "6f21f3c31fec4c7290c551f54c67a0cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ded6c48929264aac844be5332d48c8aa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5082df8250f2490793ae4ef2f199da19",
            "value": "tokenizer.model:â€‡100%"
          }
        },
        "708be012414b4279a2fedfd39a03acac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72d4f8be0ff54355a7cec30cf9be09fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76c59403e82447ecb393aac16d665158": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78949efd2d01419dace7c425a2709b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cc8e1de06c74562aef8b1214eac2ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063f95215ce24d71b297f4c4360e6225",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e905ab3258894707b8e2e0468f79da50",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "7f6f928a1ced44e6ab1356f99ba36558": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80440b8e7e6c410ba73218ebb3132860": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81514250d1a9423da15d275e2f2026d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82b7da1cf0714fcf91f48fd402cf6831": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8375c78d595645cfaeb49b70fa3655c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8384043d99874338b3470d281ad11188": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8476aeb8c37842f98033e718d6e49898": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "851b8a34172d437a87bd7b9b88b12193": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "867c131d0fcf4700a75a95e4d4c375b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87965ed43d0049abbf8487fb4044499c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88e5ea8ea7e344609418034a6337a44b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dd18e28c177447188345dfbf22db8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14b3cd3a7f2f4770b15cf4b044bc139c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_81514250d1a9423da15d275e2f2026d2",
            "value": "â€‡9.84G/?â€‡[01:26&lt;00:00,â€‡898MB/s]"
          }
        },
        "8fe3dc385d894955bb64c0c3daf723b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10fdbcdcca73414da52f6db03054aece",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5135a507e07744eea8549164aae45c22",
            "value": "â€‡11989/11989â€‡[00:13&lt;00:00,â€‡1416.71â€‡examples/s]"
          }
        },
        "90467e60959a49ecb016e8b7040b7e18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c65e600a164e13abcf0f34abe817a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9752169445144247bbaab60d8234ea93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a847cb0e19c64554832cafb7794c2169",
            "max": 6130708044,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0883a7439dd469aa0ecb61be34939f2",
            "value": 6130707460
          }
        },
        "975ce87429314387b4fca236ebcfca61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_feede8689d514e1ba0daf3bb0cf63ce0",
              "IPY_MODEL_6538fc08435e449694e110e94fd760c1",
              "IPY_MODEL_e2efff78e8f24c419ad790f5ce528a5f"
            ],
            "layout": "IPY_MODEL_26ee0f8c8739433b87383d27def8188d"
          }
        },
        "97dae5b1902d4d3ba163e60094e14cfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "991ad7e9c0b1436f9658329e5bc58f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80440b8e7e6c410ba73218ebb3132860",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_851b8a34172d437a87bd7b9b88b12193",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "9d83d1457b4c40f68630c65c7628d11f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f8a28665b7d4b049e61b3c54ba90e73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1ea12d52a734a8380db5ef0f5fdec28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_415d5bd0b30f4c3daea62bec20e8242f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef2eaca87f2a4e8f8f74a4772ea5a09c",
            "value": "â€‡11989/11989â€‡[00:01&lt;00:00,â€‡11039.34â€‡examples/s]"
          }
        },
        "a41c6110e6dd418c8bdc720c21491c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7cc8e1de06c74562aef8b1214eac2ca6",
              "IPY_MODEL_1b45412bbd664b37b2c5db628c74e9e1",
              "IPY_MODEL_0956649daa4e4a33a0637955ceb3af4d"
            ],
            "layout": "IPY_MODEL_329935d1a4d74bad95c68c8bfa72d0a3"
          }
        },
        "a5675d7e6ac64961aba2dae32245d3b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a847cb0e19c64554832cafb7794c2169": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa980ef1d818481297d1670c0cd2060e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e46d56d74db04e9e9919cad8a9dd6b2e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2703a6d091f147ebaf3ba55e90939dfa",
            "value": "100%"
          }
        },
        "abd40393f5354cbeb2c53e33c56d87c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f47f0a94d184993a2f85a7c9ca44271",
            "max": 1764113,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e754ccb651a045b3a7ad7ba01a1719ad",
            "value": 1764113
          }
        },
        "add1218f050c4b87bd3c836e564e136a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_041ef4cee1c8404389d65eddcc1f0a79",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_01f2d0aadcc34dbaa6db45126e7f0a9b",
            "value": "â€‡17.5M/17.5Mâ€‡[00:00&lt;00:00,â€‡42.7MB/s]"
          }
        },
        "adf3170238e04fc8a6dc993e47a7962b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56b875834e34ebe81df3fb1ca00e06a",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4ed10971d33485d8c2d58ad16f8b98a",
            "value": 4241003
          }
        },
        "aec3cabe68054801b3bde5aa24e90759": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af6a61f055474681b43bfba5e6cdd21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa980ef1d818481297d1670c0cd2060e",
              "IPY_MODEL_bbc49154016a49e68d7159b5a52fb4d9",
              "IPY_MODEL_be50c673005a43de91e49ec7cbee2d1d"
            ],
            "layout": "IPY_MODEL_bbe25740523a4b2f8894c048632c82e4"
          }
        },
        "b07674343c434449a129e150c7ea6a30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b11f434f0dee4c59bf5ba578ae73293b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1a7410a679d47149e7ff6e5a3c7c453": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b35fb25485304c46a32536c9d7b76be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b595281ea8eb4527b5b47d26d6fe96e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f21f3c31fec4c7290c551f54c67a0cb",
              "IPY_MODEL_adf3170238e04fc8a6dc993e47a7962b",
              "IPY_MODEL_5e43b7ae820b47ccbeb275fff80876a1"
            ],
            "layout": "IPY_MODEL_2c0e825947f74d2fb4a55ba0af11664e"
          }
        },
        "b99b0592a528481ebfcdae0d2c662247": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb578bc7bba7408a9c022ef411edb0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb7249b8b40b489b934c31638afb166a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbc49154016a49e68d7159b5a52fb4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5675d7e6ac64961aba2dae32245d3b5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8375c78d595645cfaeb49b70fa3655c0",
            "value": 1
          }
        },
        "bbe25740523a4b2f8894c048632c82e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0cf86bb9cc4cd587b32865f966642a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23830c112a784c429c087b7615558a7a",
              "IPY_MODEL_58fc1e95a1674f54b055343605260948",
              "IPY_MODEL_cabdd72ad80c4a468fe45256d2a76557"
            ],
            "layout": "IPY_MODEL_d8de2f964c0b42ab8edc05b0e0a58430"
          }
        },
        "bd8a75aa002e499f99092e0c1b5f57f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39d6be827aa64d6e9b1e3d7d55033333",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c28a257b625940c8935732051fb21305",
            "value": "â€‡11989/11989â€‡[00:00&lt;00:00,â€‡8563.44â€‡examples/s]"
          }
        },
        "be50c673005a43de91e49ec7cbee2d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ab4050223e046eeae80de31713edfb9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_250e03adeaff4799b76616c5d626b900",
            "value": "â€‡1/1â€‡[01:26&lt;00:00,â€‡86.67s/it]"
          }
        },
        "c0c4aa3cd39b43899605f73dcd2d1459": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1aa0730f9d543d6bcc46944010bf38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0871295f2674d8b88043b8ad2426cbb",
              "IPY_MODEL_159b71d1e4814e5da5c3aa382ad105ac",
              "IPY_MODEL_8dd18e28c177447188345dfbf22db8b8"
            ],
            "layout": "IPY_MODEL_4b1a5f524805476d98be6a20c827ffce"
          }
        },
        "c1cffd8c2d75427ab5fdf6d280cef2ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c21ca3aa13414ffeb39e68878e9e3ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_867c131d0fcf4700a75a95e4d4c375b7",
            "max": 11989,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78949efd2d01419dace7c425a2709b53",
            "value": 11989
          }
        },
        "c28a257b625940c8935732051fb21305": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3b6afd0d1b24406bb7e5ebd234d624c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c82d5b1f1ea04e4697624601a83308ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352a9b4eef5b414787eb87b8b1983c57",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_503adcd44b9948c7ba9a47345d08b629",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "ca0e08570b784c878e49c141239a7089": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cabdd72ad80c4a468fe45256d2a76557": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1cffd8c2d75427ab5fdf6d280cef2ba",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ccec976c24eb40d8aa959766e99983a3",
            "value": "â€‡571/571â€‡[00:00&lt;00:00,â€‡43.7kB/s]"
          }
        },
        "cc42a293ae70496a873b9067756520fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccec976c24eb40d8aa959766e99983a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdcfff298ba445ddbc1fad739d003cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1e305724c21436eb29f8d3adab15b43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f03d6351604849b4a74dfcbb6cfd52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f423b836fa54e229fd971bc1b91d7cf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef6e8f97c4e84aebad4838fb6728205c",
            "value": "100%"
          }
        },
        "d56b875834e34ebe81df3fb1ca00e06a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d84311ce64b04f1cb8289f5913a8f3db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c9239698b2b4a6ebca119a9be635b35",
              "IPY_MODEL_1afd013244ba4d0288027e175de5b374",
              "IPY_MODEL_8fe3dc385d894955bb64c0c3daf723b1"
            ],
            "layout": "IPY_MODEL_085150c68013475aa4f9cc1a2659b4f5"
          }
        },
        "d8de2f964c0b42ab8edc05b0e0a58430": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db4d663c42a249f4b4146ae1c72fdf7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f054ab5996de4f1393ea688ffacb88ab",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cdcfff298ba445ddbc1fad739d003cab",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "dec514a776774217ad4f1bcfd78a6442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ded6c48929264aac844be5332d48c8aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfbf34b7cabb452aa1bd4275ad5ca478": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c82d5b1f1ea04e4697624601a83308ad",
              "IPY_MODEL_9752169445144247bbaab60d8234ea93",
              "IPY_MODEL_56dbb978bc054c74b87b8a7cfcc01627"
            ],
            "layout": "IPY_MODEL_4301e4c73e964e35bb836df27a476141"
          }
        },
        "dffffc4858c842acb1b692a8b7f3b82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_152d2743aa7747548871d293ebae1bb7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0bb68ef07ba14a41b2a8c25d47458586",
            "value": "Map:â€‡100%"
          }
        },
        "e1a83d9c10f54e28b318e05f575f2708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4f03d6351604849b4a74dfcbb6cfd52",
              "IPY_MODEL_0f4098aa9dec44f08ed235b1502849b6",
              "IPY_MODEL_211dba39ce344a83a77f936203b56498"
            ],
            "layout": "IPY_MODEL_4bdc189a1e9f4397b07371e067b9a179"
          }
        },
        "e2efff78e8f24c419ad790f5ce528a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f891fc5a851946daa9ff6490ef074ded",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4c4b3295658e49e3b3907ef93d02d76c",
            "value": "â€‡5.78G/?â€‡[00:51&lt;00:00,â€‡1.15GB/s]"
          }
        },
        "e46d56d74db04e9e9919cad8a9dd6b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e61e9a275c46493f9ea3c44f82afe097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90467e60959a49ecb016e8b7040b7e18",
            "max": 11989,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dec514a776774217ad4f1bcfd78a6442",
            "value": 11989
          }
        },
        "e74859d955144b3e975991245ebcbb51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e754ccb651a045b3a7ad7ba01a1719ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7aa18171eed429a9d47ca112031c6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_991ad7e9c0b1436f9658329e5bc58f8a",
              "IPY_MODEL_3b2001049d094d21b777d822fd67eb78",
              "IPY_MODEL_6d362fbf445045389a19c301caaa0d48"
            ],
            "layout": "IPY_MODEL_6bebe9bf15d84ca6a2ca01e869449c5b"
          }
        },
        "e905ab3258894707b8e2e0468f79da50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb3b328104954285bf694de63e6c0d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d41e65fe3624c57a4c42bcdb4e99a7d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6170800399c248c3b6c83ada8754bd21",
            "value": "â€‡1.76M/1.76Mâ€‡[00:00&lt;00:00,â€‡7.98MB/s]"
          }
        },
        "ef2eaca87f2a4e8f8f74a4772ea5a09c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef6e8f97c4e84aebad4838fb6728205c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f054ab5996de4f1393ea688ffacb88ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0871295f2674d8b88043b8ad2426cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b99b0592a528481ebfcdae0d2c662247",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_40da6317ee3a4419b893ab20ed61b1a0",
            "value": "unsloth.Q8_0.gguf:â€‡"
          }
        },
        "f0883a7439dd469aa0ecb61be34939f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4d11098ccc049558178fea91da32637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4ed10971d33485d8c2d58ad16f8b98a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f784f27dd5554d4ab36f533e6251d259": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_268d6b2a547542a7a5daf679c4f40f7f",
            "max": 17525357,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d83d1457b4c40f68630c65c7628d11f",
            "value": 17525357
          }
        },
        "f8723223377b4d09a00e55d06f3f703c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f891fc5a851946daa9ff6490ef074ded": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4a36ca0dc84195add7474f718b3620": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa99e8d6359a4e97a37a27e538b9a7c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "feede8689d514e1ba0daf3bb0cf63ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aec3cabe68054801b3bde5aa24e90759",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b11f434f0dee4c59bf5ba578ae73293b",
            "value": "unsloth.Q4_K_M.gguf:â€‡"
          }
        },
        "ff717cf5ae304c5d94c867a4d3fde006": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dffffc4858c842acb1b692a8b7f3b82b",
              "IPY_MODEL_e61e9a275c46493f9ea3c44f82afe097",
              "IPY_MODEL_a1ea12d52a734a8380db5ef0f5fdec28"
            ],
            "layout": "IPY_MODEL_1e1eee19e4124d1eb95a6a83e7e596c1"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
