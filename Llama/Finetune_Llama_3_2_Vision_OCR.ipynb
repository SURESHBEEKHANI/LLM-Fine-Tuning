{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjvbM0OrlwF6"
      },
      "source": [
        " **Fine-tuning LLaMA 3.2 Vision on the DataStudio/OCR_handwritting_HAT2023 dataset using Unsloth requires several steps, including dataset preparation, model loading, training, and pushing the model to Hugging Face. Below is a structured approach to fine-tune the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L-0cn_CjkEE"
      },
      "source": [
        "### 1.Install and upgrade UnSloth library for optimized model training\n",
        "\n",
        "- Use `%%capture` to suppress installation output in Jupyter/Colab environments.\n",
        "- Install the `unsloth` package from PyPI for initial setup.\n",
        "- Uninstall the existing `unsloth` package to ensure a clean installation.\n",
        "- Upgrade to the latest version of `unsloth` directly from the GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bDTd1MSsVnY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# The `%%capture` magic in Jupyter/Colab captures output, suppressing it from being displayed.\n",
        "\n",
        "# Install the `unsloth` package from PyPI\n",
        "!pip install unsloth\n",
        "\n",
        "# Uninstall `unsloth` to ensure a clean installation, then install the latest version from GitHub\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt22_B_djphz"
      },
      "source": [
        "### 2. Load the model\n",
        "\n",
        "\n",
        "Load the `unsloth/Llama-3.2-11B-Vision-Instruct` model using FastVisionModel.\n",
        "\n",
        "Enable 4-bit quantization to reduce memory usage.\n",
        "Utilize UnSloth's gradient checkpointing for efficient training and inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1WT3bF2eVZW"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6jAtJ2rjvmT"
      },
      "source": [
        "### 3. Add LoRA fine-tuning configuration for Llama 3.2 Vision model\n",
        "\n",
        "- Apply LoRA (Low-Rank Adaptation) to fine-tune the vision and language components of the model.\n",
        "- Enable fine-tuning for vision layers, language layers, attention modules, and MLP modules.\n",
        "- Configure LoRA parameters such as rank (`r`), alpha (`lora_alpha`), and dropout (`lora_dropout`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4ZmTZPag4UY"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "    r                          = 16,\n",
        "    lora_alpha                 = 16,\n",
        "    lora_dropout               = 0,\n",
        "    bias                       = \"none\",\n",
        "    random_state               = 3407,\n",
        "    use_rslora                 = False,\n",
        "    loftq_config               = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNzA6auNmS6I"
      },
      "source": [
        "### 4. Load a Subset of the OCR Handwriting Dataset for Vision-Language Tasks\n",
        "\n",
        "Load the DataStudio/OCR_handwritting_HAT2023 dataset using the Hugging Face datasets library.\n",
        "Select a subset of the training data (first 500 samples) for faster experimentation and prototyping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qgK1uTrg8qA"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"DataStudio/OCR_handwritting_HAT2023\",\n",
        "                       split=\"train[0:500]\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vacmURNLiajN"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset[45][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD2Hg7Efmt31"
      },
      "source": [
        "### **5. Convert OCR Dataset into Conversation Format for Vision-Language Models**\n",
        "\n",
        "Define a system instruction for extracting text from images using Optical Character Recognition (OCR).\n",
        "Implement a convert_to_conversation function to transform dataset samples into a structured conversation format suitable for Vision-Language Models (VLMs).\n",
        "Apply the transformation to the dataset to create a new converted_dataset.\n",
        "Code Implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7uQ8CPSha0H"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"\n",
        "You are an expert in Optical Character Recognition (OCR). Your task is to accurately extract and transcribe text from images while preserving the original structure, formatting, and readability. Ensure precision in capturing all characters, words, and symbols, minimizing errors and distortions.\n",
        "\"\"\"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "    \"\"\"\n",
        "    Converts a dataset sample into a structured conversation format for OCR fine-tuning.\n",
        "\n",
        "    Parameters:\n",
        "    sample (dict): A dictionary containing an image and its corresponding extracted text.\n",
        "\n",
        "    Returns:\n",
        "    dict: A structured conversation containing user input (instruction + image)\n",
        "          and assistant output (extracted text).\n",
        "    \"\"\"\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": instruction},\n",
        "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}],\n",
        "        },\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "\n",
        "# Convert the entire dataset into conversation format\n",
        "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkq5VVsDj8y0"
      },
      "source": [
        "### 6. Enable inference on Llama 3.2 Vision model for generating product descriptions\n",
        "\n",
        "- Prepare the model for inference using `FastVisionModel.for_inference`.\n",
        "- Generate a product description by processing an image and instruction using the Vision-Language Model.\n",
        "- Use a streaming approach to display generated text in real-time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcfvfdHbiymF"
      },
      "outputs": [],
      "source": [
        "\n",
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image = dataset[45][\"image\"]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True\n",
        ")\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYPrrtxkMbq"
      },
      "source": [
        "### 7. Set up SFTTrainer for fine-tuning Llama 3.2 Vision model on product descriptions\n",
        "\n",
        "- Enable the model for training using `FastVisionModel.for_training`.\n",
        "- Configure the `SFTTrainer` from the `trl` library for supervised fine-tuning (SFT) with UnSloth optimizations.\n",
        "- Use `UnslothVisionDataCollator` to handle multi-modal data efficiently during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVMmJJhmi6TM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model)  # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
        "    train_dataset=converted_dataset,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=30,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16=is_bf16_supported(),\n",
        "        logging_steps=5,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # For Weights and Biases\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc=4,\n",
        "        max_seq_length=2048,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZuRDMGki9c-"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MEPTCKHnXXs"
      },
      "source": [
        "### **Export and save the model to your Hugging Face account**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdNtNNOyna9V"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub_merged(\n",
        "    \"SURESHBEEKHANI/Finetune_Llama_3_2_Vision_OCR\",\n",
        "    tokenizer,\n",
        "    token=\"\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
