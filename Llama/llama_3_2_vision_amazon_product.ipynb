{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/llama_3_2_vision_amazon_product.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "443Pt2LEIwmo"
      },
      "source": [
        "### Fine-Tuning Llama 3.2 Vision\n",
        "**fine-tune a multimodal model by Meta AI on the Amazon product dataset using the Unsloth framework**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB-5a2vvJg9L"
      },
      "source": [
        "### 1.Install and upgrade UnSloth library for optimized model training\n",
        "\n",
        "- Use `%%capture` to suppress installation output in Jupyter/Colab environments.\n",
        "- Install the `unsloth` package from PyPI for initial setup.\n",
        "- Uninstall the existing `unsloth` package to ensure a clean installation.\n",
        "- Upgrade to the latest version of `unsloth` directly from the GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bDTd1MSsVnY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZGiemxYJE4W"
      },
      "source": [
        "### 2. Load the model\n",
        "\n",
        "\n",
        "Load the `unsloth/Llama-3.2-11B-Vision-Instruct` model using FastVisionModel.\n",
        "\n",
        "Enable 4-bit quantization to reduce memory usage.\n",
        "Utilize UnSloth's gradient checkpointing for efficient training and inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnQHDnjrGEe4"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel  # Use FastLanguageModel for LLMs if needed\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
        "    load_in_4bit=True,  # Use 4-bit precision to reduce memory usage; set to False for 16-bit LoRA\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Use \"unsloth\" or True for long context support\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7q0LSXRJaPD"
      },
      "source": [
        "### 3. Add LoRA fine-tuning configuration for Llama 3.2 Vision model\n",
        "\n",
        "- Apply LoRA (Low-Rank Adaptation) to fine-tune the vision and language components of the model.\n",
        "- Enable fine-tuning for vision layers, language layers, attention modules, and MLP modules.\n",
        "- Configure LoRA parameters such as rank (`r`), alpha (`lora_alpha`), and dropout (`lora_dropout`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug0jCsRIGKB4"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "    r                          = 16,\n",
        "    lora_alpha                 = 16,\n",
        "    lora_dropout               = 0,\n",
        "    bias                       = \"none\",\n",
        "    random_state               = 3407,\n",
        "    use_rslora                 = False,\n",
        "    loftq_config               = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL6EyEBSJo8s"
      },
      "source": [
        "### 4.  Load a subset of the Amazon Product Descriptions dataset for Vision-Language tasks\n",
        "\n",
        "- Load the `philschmid/amazon-product-descriptions-vlm` dataset using the Hugging Face `datasets` library.\n",
        "- Select a subset of the training data (first 500 samples) for faster experimentation and prototyping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phKh9rQ-GPCo"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"philschmid/amazon-product-descriptions-vlm\",\n",
        "                       split=\"train[0:500]\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_1fqY1BGZKw"
      },
      "outputs": [],
      "source": [
        "dataset[45][\"description\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM6Sc6UkJtUC"
      },
      "source": [
        "### 5.  Convert Amazon product descriptions dataset into conversation format for Vision-Language Models\n",
        "\n",
        "- Define a system instruction for generating product descriptions based on images.\n",
        "- Implement a `convert_to_conversation` function to transform dataset samples into a conversation-like structure suitable for Vision-Language Models (VLMs).\n",
        "- Apply the transformation to the dataset to create a new `converted_dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23ep8xCbGbjR"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"\n",
        "You are an expert Amazon worker who is good at writing product descriptions.\n",
        "Write the product description accurately by looking at the image.\n",
        "\"\"\"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": instruction},\n",
        "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": sample[\"description\"]}],\n",
        "        },\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "\n",
        "# Apply the conversion to each sample in the dataset\n",
        "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmDvSWdYKCZC"
      },
      "source": [
        "### 6. Enable inference on Llama 3.2 Vision model for generating product descriptions\n",
        "\n",
        "- Prepare the model for inference using `FastVisionModel.for_inference`.\n",
        "- Generate a product description by processing an image and instruction using the Vision-Language Model.\n",
        "- Use a streaming approach to display generated text in real-time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWbxAUh2J6vG"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image = dataset[45][\"image\"]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True\n",
        ")\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVyHtFP2KJPq"
      },
      "source": [
        "### 7. Set up SFTTrainer for fine-tuning Llama 3.2 Vision model on product descriptions\n",
        "\n",
        "- Enable the model for training using `FastVisionModel.for_training`.\n",
        "- Configure the `SFTTrainer` from the `trl` library for supervised fine-tuning (SFT) with UnSloth optimizations.\n",
        "- Use `UnslothVisionDataCollator` to handle multi-modal data efficiently during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNFYowCcIZml"
      },
      "outputs": [],
      "source": [
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model)  # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
        "    train_dataset=converted_dataset,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=30,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16=is_bf16_supported(),\n",
        "        logging_steps=5,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # For Weights and Biases\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc=4,\n",
        "        max_seq_length=2048,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwnUDTMtKlzB"
      },
      "source": [
        "### 8. Training the model\n",
        "Start the training process by running the trainer.train() code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vSgM_IzKlhi"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFIx3aHuk1ka"
      },
      "source": [
        "### 11.Push the trained model to Hugging Face Model Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1lbdXL5Qe3U3"
      },
      "outputs": [],
      "source": [
        "# Export and save the model to your Hugging Face account\n",
        "model.push_to_hub_merged(\n",
        "    \"SURESHBEEKHANI/llama_3.2_vision_amazon_product_description\",\n",
        "    tokenizer,\n",
        "    token=\"\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
