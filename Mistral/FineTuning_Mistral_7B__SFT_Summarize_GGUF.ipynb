{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/FineTuning_Mistral_7B__SFT_Summarize_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPhTxO0YXNB7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# The `%%capture` magic in Jupyter/Colab captures output, suppressing it from being displayed.\n",
        "\n",
        "# Install the `unsloth` package from PyPI\n",
        "!pip install unsloth\n",
        "\n",
        "# Uninstall `unsloth` to ensure a clean installation, then install the latest version from GitHub\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaH0Mdp5XFtD"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel  # Importing the FastLanguageModel class from the unsloth library\n",
        "import torch  # Importing PyTorch for handling tensors and computations\n",
        "\n",
        "# Set the maximum sequence length for the model's input\n",
        "max_seq_length = 2048  # The maximum number of tokens the model can process in one sequence. Customize as needed.\n",
        "# Note: The library internally supports RoPE (Rotary Position Embedding) scaling to handle long sequences.\n",
        "\n",
        "# Set the data type for model computation\n",
        "dtype = None  # Automatically detect the best precision.\n",
        "# Set dtype to 'torch.float16' for Tesla T4/V100 GPUs, or 'torch.bfloat16' for Ampere and newer GPUs.\n",
        "\n",
        "# Choose whether to use 4-bit quantization for the model\n",
        "load_in_4bit = True  # Enabling 4-bit quantization reduces memory usage and speeds up computation.\n",
        "# Set to False if higher precision is needed or memory is not a concern.\n",
        "\n",
        "# Load the model and tokenizer using the FastLanguageModel class\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/mistral-7b-v0.3\",  # Specify the model name to load. Replace with any model of your choice.\n",
        "    max_seq_length=max_seq_length,  # Pass the chosen maximum sequence length.\n",
        "    dtype=dtype,  # Pass the chosen data type for computations.\n",
        "    load_in_4bit=load_in_4bit,  # Pass whether to use 4-bit quantization.\n",
        ")\n",
        "\n",
        "# Explanation:\n",
        "# - `FastLanguageModel.from_pretrained` is a convenient method to load both the model and tokenizer.\n",
        "# - `model_name`: The name of the pre-trained model. Example: \"unsloth/mistral-7b-v0.3\".\n",
        "# - `max_seq_length`: Configures the maximum token length the model can handle in one input.\n",
        "# - `dtype`: Allows precise control over computation precision for optimal performance on different hardware.\n",
        "# - `load_in_4bit`: If True, enables 4-bit quantization to reduce memory footprint while maintaining good accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spUJjfhNz8_j"
      },
      "source": [
        "### We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOevBwVblpJi"
      },
      "outputs": [],
      "source": [
        "# Configure the model with PEFT (Parameter-Efficient Fine-Tuning) settings using LoRA (Low-Rank Adaptation)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The base model to be fine-tuned using PEFT techniques\n",
        "\n",
        "    # Low-Rank Adaptation (LoRA) rank\n",
        "    r=16,  # Defines the rank of the low-rank matrices. Common choices: 8, 16, 32, 64, 128.\n",
        "    # Larger values increase expressiveness but require more memory.\n",
        "\n",
        "    # Modules to target for LoRA fine-tuning\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projection layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "    ],\n",
        "    # Only these specified modules will be fine-tuned to reduce memory and computational overhead.\n",
        "\n",
        "    # LoRA-specific hyperparameters\n",
        "    lora_alpha=16,  # Scaling factor for LoRA weights. Balances new and pre-trained weights.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA. Setting to 0 often gives optimized performance.\n",
        "\n",
        "    # Bias handling in fine-tuning\n",
        "    bias=\"none\",  # Specifies bias tuning. \"none\" is optimized for performance. Alternatives: \"all\", \"lora_only\".\n",
        "\n",
        "    # Optimizations for VRAM and context length\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Use gradient checkpointing to save memory during training.\n",
        "    # The \"unsloth\" setting reduces VRAM usage by ~30%, allowing larger batch sizes or longer contexts.\n",
        "\n",
        "    # Random seed for reproducibility\n",
        "    random_state=3407,  # Ensures the results are reproducible across runs.\n",
        "\n",
        "    # Advanced fine-tuning features\n",
        "    use_rslora=False,  # Enables Rank-Stabilized LoRA (rSLoRA) if set to True. Useful for stability in high ranks.\n",
        "    loftq_config=None,  # Configures LoftQ (Low Overhead Fine-Tuning Quantization), if used. Set to None for default.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt_WTHiY1mlY"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep and Load\n",
        "We now use the text-summarizer dataset from [SURESHBEEKHANI](https://huggingface.co/datasets/SURESHBEEKHANI/text-summarizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DY2jFcem1Av"
      },
      "outputs": [],
      "source": [
        "# Define a string template for the prompt format used for generating responses\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# EOS_TOKEN is the special token that signifies the end of a sequence. It ensures the model stops generating when it reaches this token.\n",
        "EOS_TOKEN = tokenizer.eos_token  # Retrieve the EOS token from the tokenizer (ensures proper stopping in generation)\n",
        "\n",
        "# Define the formatting function for processing the dataset\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]  # Extract instructions from the dataset\n",
        "    inputs = examples[\"dialogue\"]  # Extract dialogues (context) from the dataset\n",
        "    outputs = examples[\"summary\"]  # Extract expected summaries from the dataset\n",
        "\n",
        "    texts = []  # Initialize an empty list to store the formatted text prompts\n",
        "    # Loop over the instructions, inputs (dialogues), and outputs (summaries) to create the full prompt\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Use the alpaca_prompt template to format each instruction, input, and output\n",
        "        # Ensure EOS_TOKEN is appended to avoid endless generation during model training or inference\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)  # Append the formatted text to the list\n",
        "\n",
        "    # Return the formatted text wrapped in a dictionary with the key \"text\" for use in model training\n",
        "    return { \"text\": texts }\n",
        "\n",
        "# Load the dataset from the \"SURESHBEEKHANI/text-summarizer\" dataset repository\n",
        "# 'split=\"train\"' indicates we are loading the training data.\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"SURESHBEEKHANI/text-Summarize\", split=\"train\")\n",
        "\n",
        "# Apply the formatting function to the dataset in batches, preparing the data for model training\n",
        "# 'batched=True' means the function will process multiple examples at once, improving efficiency.\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE4pAaekptdz"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa0hTI6jyhG1"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbtx0xgNynxV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and modules\n",
        "from trl import SFTTrainer  # SFTTrainer is a class used for training models using Supervised Fine-Tuning (SFT)\n",
        "from transformers import TrainingArguments  # TrainingArguments holds the configuration for model training\n",
        "from unsloth import is_bfloat16_supported  # Utility to check if bfloat16 is supported in the system\n",
        "\n",
        "# Initialize the SFTTrainer with several parameters for training\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # The model to be trained (not defined here, assumed to be defined elsewhere)\n",
        "    tokenizer=tokenizer,  # The tokenizer to be used for text preprocessing (assumed to be defined elsewhere)\n",
        "    train_dataset=dataset,  # The dataset used for training (assumed to be defined elsewhere)\n",
        "    dataset_text_field=\"text\",  # Field in the dataset containing the input text\n",
        "    max_seq_length=max_seq_length,  # Maximum sequence length for tokenized inputs (assumed to be defined elsewhere)\n",
        "    dataset_num_proc=2,  # Number of processes to use for data preprocessing; can speed up data loading\n",
        "    packing=False,  # Whether to use sequence packing, which can make training faster for short sequences\n",
        "    args=TrainingArguments(  # Set the training arguments and hyperparameters\n",
        "        per_device_train_batch_size=2,  # Batch size per device (e.g., GPU or CPU) during training\n",
        "        gradient_accumulation_steps=4,  # Number of steps before performing a gradient update (helps with large batch sizes)\n",
        "        warmup_steps=5,  # Number of steps for the learning rate warmup before it starts decaying\n",
        "        max_steps=60,  # Total number of training steps. Usually corresponds to num_train_epochs * num_steps_per_epoch\n",
        "        learning_rate=2e-4,  # Learning rate for the optimizer\n",
        "        fp16=not is_bfloat16_supported(),  # Use mixed-precision (float16) if bfloat16 is not supported by the hardware\n",
        "        bf16=is_bfloat16_supported(),  # Use bfloat16 if supported by the hardware\n",
        "        logging_steps=1,  # Frequency (in steps) to log training metrics (e.g., loss) during training\n",
        "        optim=\"adamw_8bit\",  # Optimizer type. Using AdamW with 8-bit precision for memory efficiency\n",
        "        weight_decay=0.01,  # Weight decay parameter for regularization to prevent overfitting\n",
        "        lr_scheduler_type=\"linear\",  # Learning rate scheduler type; here it's set to linear decay\n",
        "        seed=3407,  # Random seed for reproducibility of results\n",
        "        output_dir=\"outputs\",  # Directory where the model checkpoints and outputs will be saved\n",
        "        report_to=\"none\",  # Specify where to report metrics (e.g., use \"wandb\" for reporting to Weights & Biases)\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The trainer is now set up and can be used for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_SPkOg3yvA8"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAWT7FkLy0Bc"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLZNYl4O3xMj"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqXQvpqd341z"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLW2gbbS34kD"
      },
      "outputs": [],
      "source": [
        "# Define the prompt template for text summarization.\n",
        "alpaca_prompt = \"\"\"Below is a passage of text. Write a concise summary of the text below.\n",
        "\n",
        "### Text:\n",
        "{}\n",
        "\n",
        "### Summary:\n",
        "{}\"\"\"  # The summary part is left empty for generation.\n",
        "\n",
        "# FastLanguageModel.for_inference(model) enables optimizations for faster inference.\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference by configuring the model for efficient use\n",
        "\n",
        "# Example of a text summarization task.\n",
        "# Here, you provide a longer piece of text as input, and the model will generate a concise summary.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "         alpaca_prompt.format(  # Format the prompt with the input text and an empty placeholder for the summary.\n",
        "            \"\"\"Artificial intelligence (AI) refers to machines demonstrating intelligence, in contrast to natural intelligence in humans and animals.\n",
        "            Leading AI textbooks define the field as the study of \"intelligent agents\" – devices that perceive their environment and act to achieve goals.\n",
        "            Often, \"artificial intelligence\" is used to describe machines mimicking cognitive functions like learning and problem-solving.\"\"\"\n",
        "            ,  # Insert input text for summarization.\n",
        "            \"\"  # The summary section is empty for the model to fill in.\n",
        "        )\n",
        "    ], return_tensors=\"pt\"  # Convert input to PyTorch tensors.\n",
        ").to(\"cuda\")  # Move the input data to the GPU for faster processing.\n",
        "\n",
        "# Generate the summary using the model.\n",
        "# 'max_new_tokens' controls how many tokens the model is allowed to generate.\n",
        "# 'use_cache' allows for faster generation by caching previous results.\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "# Decode the generated tokens back into readable text.\n",
        "# This will give us the model's summary of the provided input text.\n",
        "tokenizer.batch_decode(outputs)  # Convert the output tokens to text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10YzIGxp5KnD"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzJnADOp5Jnx"
      },
      "outputs": [],
      "source": [
        "# Define a prompt template for text summarization.\n",
        "# This template will guide the model to generate a concise summary of the given text.\n",
        "alpaca_prompt = \"\"\"Below is a passage of text. Please provide a brief summary.\n",
        "\n",
        "### Text:\n",
        "{}\n",
        "\n",
        "### Summary:\n",
        "{}\"\"\"  # The summary placeholder will be filled by the model.\n",
        "\n",
        "# Optimize the model for faster inference.\n",
        "# This process enhances the model’s efficiency by reducing latency and improving GPU performance.\n",
        "FastLanguageModel.for_inference(model)  # Enables 2x faster inference by configuring the model for optimized use.\n",
        "\n",
        "# Tokenize the input text for summarization.\n",
        "# The example text provided will be used as input for summarization.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(  # Format the prompt with the input text and an empty placeholder for the summary.\n",
        "            \"\"\"Artificial intelligence (AI) refers to machines demonstrating intelligence, in contrast to natural intelligence in humans and animals.\n",
        "            Leading AI textbooks define the field as the study of \"intelligent agents\" – devices that perceive their environment and act to achieve goals.\n",
        "            Often, \"artificial intelligence\" is used to describe machines mimicking cognitive functions like learning and problem-solving.\"\"\"\n",
        "            ,  # Insert input text for summarization.\n",
        "            \"\"  # The summary section is empty for the model to fill in.\n",
        "        )\n",
        "    ], return_tensors=\"pt\"  # Convert input text into PyTorch tensors for model processing.\n",
        ").to(\"cuda\")  # Move data to the GPU for quicker processing.\n",
        "\n",
        "# Import TextStreamer to stream the summary generation token by token.\n",
        "# This will allow real-time generation of summaries, especially for long input texts.\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Initialize the TextStreamer to decode the generated tokens during streaming.\n",
        "# This facilitates immediate feedback on the model’s output.\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Generate the summary using the model, streaming token-by-token for faster results.\n",
        "# The model will produce a summary up to a maximum of 128 tokens.\n",
        "_ = model.generate(\n",
        "    **inputs,  # Provide the tokenized input text to the model.\n",
        "    streamer=text_streamer,  # Enable token-by-token streaming.\n",
        "    max_new_tokens=128  # Limit the number of tokens in the generated summary.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7j1aCbF5-hY"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5LDZmpW5-QZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqPOJ7yU5ikI"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inOFYfZF598K"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF4T-jfE6Nn0"
      },
      "outputs": [],
      "source": [
        "# Import FastLanguageModel from the unsloth library.\n",
        "# This class allows you to load pre-trained models, configure them for fast inference, and perform tasks like text generation.\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the pre-trained model and tokenizer using the FastLanguageModel class.\n",
        "# - 'model_name' is the name of the model you trained (in this case, \"lora_model\").\n",
        "# - 'max_seq_length' is the maximum sequence length the model can handle for input.\n",
        "# - 'dtype' is the data type for model weights (such as float32 or float16).\n",
        "# - 'load_in_4bit' specifies whether to load the model with reduced 4-bit precision for efficiency (saves memory).\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_model\",  # Replace with the name of the model you used for training.\n",
        "    max_seq_length = max_seq_length,  # Maximum length of the input sequence.\n",
        "    dtype = dtype,  # Data type for the model (e.g., float32, float16).\n",
        "    load_in_4bit = load_in_4bit,  # Option to load the model in 4-bit precision to save memory.\n",
        ")\n",
        "\n",
        "# Enable optimizations for inference to make the model's token generation up to 2x faster.\n",
        "# This improves performance by reducing latency and making the model more efficient during text generation.\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference for optimized generation.\n",
        "\n",
        "# 'alpaca_prompt' should be the pre-defined prompt template you are using to format your input for the model.\n",
        "# This template typically has placeholders for instructions, inputs, and outputs that are formatted during generation.\n",
        "\n",
        "# Tokenize the input text using the tokenizer.\n",
        "# Here we are preparing the input by filling the prompt template with specific instructions.\n",
        "# The instruction asks about a famous tall tower in Paris, and the input/output are left blank for the model to generate a response.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(  # Format the prompt with the input text and an empty placeholder for the summary.\n",
        "            \"\"\"Artificial intelligence (AI) refers to machines demonstrating intelligence, in contrast to natural intelligence in humans and animals.\n",
        "            Leading AI textbooks define the field as the study of \"intelligent agents\" – devices that perceive their environment and act to achieve goals.\n",
        "            Often, \"artificial intelligence\" is used to describe machines mimicking cognitive functions like learning and problem-solving.\"\"\"\n",
        "            ,  # Insert input text for summarization.\n",
        "            \"\"  # The summary section is empty for the model to fill in.\n",
        "        )\n",
        "    ], return_tensors=\"pt\"  # Convert the formatted input text into PyTorch tensors (required for the model to process).\n",
        ").to(\"cuda\")  # Move the input tensors to the GPU to speed up computation.\n",
        "\n",
        "# Generate the output using the model based on the tokenized inputs.\n",
        "# The model will generate a response with a maximum of 64 new tokens.\n",
        "# 'use_cache=True' allows for more efficient generation by reusing intermediate states during the generation process.\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "# Decode the generated token IDs back into human-readable text using the tokenizer.\n",
        "# 'batch_decode' converts the tokenized outputs into strings of text.\n",
        "tokenizer.batch_decode(outputs)  # Retrieve the final generated output (e.g., the model's response to the question).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQbhrTsu8T18"
      },
      "source": [
        "## Push the trained model to the Hugging Face Model Hub using the GGUF format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2DJccI5Trtv"
      },
      "outputs": [],
      "source": [
        "# Push the trained model to the Hugging Face Model Hub using the GGUF format\n",
        "model.push_to_hub_gguf(\n",
        "    \"SURESHBEEKHANI/Mistral_7B_Summarizer_SFT_GGUF\",  # Specify the model repository path on Hugging Face Hub. Replace \"hf\" with your Hugging Face username.\n",
        "    tokenizer,  # Pass the tokenizer associated with the model to ensure compatibility on the hub\n",
        "    quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],  # Specify the quantization methods to apply for optimized model storage (e.g., q4_k_m, q8_0, q5_k_m)\n",
        "    token=\"\",  # Provide the Hugging Face token for authentication. Obtain a token at https://huggingface.co/settings/tokens\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
